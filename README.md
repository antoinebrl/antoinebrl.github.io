<link rel="stylesheet" type="text/css" media="all" href="https://antoinebrl.github.io/css/style.css"/>
      <div class="container"><div class="home" id="home"><h1 class="pageTitle">Recent Posts</h1><ul class="posts noList"><li><span class="date">February 06, 2022</span><h3><a href="https://antoinebrl.github.io/blog/rl-mars-lander/">Learning to Land on Mars with Reinforcement Learning</a></h3><p>Once in a while, a friend who is either learning to code or tackling complex problems drags me back to CodinGame However, I tend to put on type of challenge aside: optimal control systems. This kind of problem often requires to hand-craft a good cost function and to model the transition dynamics. But what if we could solve the challenge without coding a control policy? This is the story of how I landed a rover on Mars with reinforcement learning.</p></li><li><span class="date">September 26, 2021</span><h3><a href="https://antoinebrl.github.io/blog/active-learning/">Why Active&nbsp;Learning is not widely adopted?</a></h3><p>I recently spent quite some time learning and playing with Active Learning. The claim of active learning is quite appealing: a model can achieve the same, or even better, performances with fewer annotated samples if it can select which specific instances need to be labeled. Sounds promising, right? In this article, I will attempt to reflect on the promises of active learning to show that the picture is not all bright. Active Learning has a few drawbacks, however, it represents a viable solution when building datasets at scale or when tackling the diversity edge cases.</p></li><li><span class="date">March 21, 2021</span><h3><a href="https://antoinebrl.github.io/blog/model-relationship/">Relationship Between Machine Learning Methods</a></h3><p>Back when I was doing my master in Data Science and Cognitive Systems at KTH I had an interesting conversion about the number of algorithms a data scientist should know to do his work.My friend was baffled by the number of techniques that we had to learn for the exams.As a rather lazy student, I was a bit perplex too. The truth is that many methods and tricks can be usedwithin widely different models.</p></li><li><span class="date">February 12, 2021</span><h3><a href="https://antoinebrl.github.io/blog/knn-variance/">K-Nearest Neighbors (KNN) - Visualizing the Variance</a></h3><p>In chapter 2 of The Elements of Statistical Learning,the authors compare least square regression and nearest neighbors in terms of bias and variance.I thought it would be interesting to have an interactive visualization to understandwhy k-NN is said to have low bias and high variance.</p></li><li><span class="date">January 27, 2021</span><h3><a href="https://antoinebrl.github.io/blog/kmeans/">Deep (Deep, Deep) Dive into K-Means Clustering</a></h3><p>Recently, my interest in outlier detection and clustering techniques spiked. The rabbit hole was deeper than anticipated! Let's first talk about <i>k-means</i> clustering. For completeness, I provide a high-level description of the algorithm, some step-by-step animations, a few equations for math lovers, and a Python implementation using Numpy. Finally, I cover the limitations and variants of <i>k-means</i>. You can scroll to the end if you want to test your understanding with some typical interview questions.</p></li><li><span class="date">May 17, 2020</span><h3><a href="https://antoinebrl.github.io/blog/conv1d/">Convolution Interactive Sandbox</a></h3><p>Play around convolutions with this interactive visualization. Choose different functions, swap kernels and explore the influences of each parameters.</p></li></ul><!-- Pagination links --><div class="pagination"></div></div></div>
