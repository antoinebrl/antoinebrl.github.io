---
layout: post
title:  "All you need to know (and more) about K-Means clustering"
date:   2021-01-27
references: kmeans.bib
toc: true
description: "Recently, I had a spike of interest in outlier detection and
clustering techniques. The rabbit hole was deeper than anticipated!
Let's first talk about <i>k-means</i> clustering.
For completeness, I provide a high-level understanding, some step-by-step
animations, a few equations for math lovers, and a Python implementation
using Numpy. Finally, I will cover the limitations and variants of <i>k-means</i>.
You can scroll to the end if you want to test your understanding with some
typical interview questions."
---

<script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/x-mathjax-config">
MathJax = {
  loader: {load: ['[tex]/color']},
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\[', '\\]']],
    packages: {'[+]': ['color']}
  },
  svg: {
    fontCache: 'glo\(( bal',
    linebreak: 
      automatic: true
  }
};
</script>
<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<script type="text/javascript" src="https://d3js.org/d3.v6.min.js"></script>
<style>
.MathJax {
  overflow-x:auto;
}
svg.animation {
  box-shadow: 0 10px 16px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19) !important;
}
figure {
  text-align: center;
  margin: 0.5rem auto 2rem auto !important;
}
</style>

<h2 class="no_toc">Introduction</h2>

**Clustering is the task of grouping similar looking data points into subsets**.
Such a group is called a cluster.
Data points within the same cluster should look alike or share similar
properties compared to items in other clusters.

**Clustering is an [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) technique**
mainly used for data exploration, data mining and information compression through quantization.
All clustering algorithms have to find patterns in the data by themselves as it comes without any labels.
The consequence is that the notion of a "good cluster" depends on the use case and is somehow subjective.
Over the years, this has led to the development of hundreds of algorithms, all with
their merits and drawbacks.

***K-means* is a centroid model**, each cluster is represented by a central point.
"K" represents the number of clusters and "mean" is the aggregation operation
applied to elements of a cluster to define the central location. 
**Central points are called *centroids* or *prototypes* of the clusters**.

<figure>
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-width:300px">
  <figcaption>Fig.1 - Illustration of centroid clustering
	<br>Black dots represent data points, coloured dots are centroids
	<br>and coloured areas show cluster expansions.</figcaption>
</figure>

## The *K-Means* Algorithm

From a theoretical perspective, finding the optimal solution for a clustering problem
is particularly difficult (NP-hard). Hence, **all algorithms are only approximations
of the optimal solution**. The *k-means* algorithm, sometimes referred to as the Lloyd
algorithm after his author {% cite lloyd1982least -f kmeans %},
is an **iterative process which refines the solution until it
converges to a local optimum**.


Let's use some mathematical notations to formalize this:
- \\( X = \\{ \textbf{x}_1, \, \ldots ,\; \textbf{x}_n ;\, \textbf{x}_j \in \mathbb{R}^d \\} \\)
denotes the set of data points of dimensions \\( d \\).
- \\( C = \\{ \textbf{c}_1, \, \ldots ,\; \textbf{c}_k ;\, \textbf{c}_i \in \mathbb{R}^d \\} \\)
is the set of centroids.
- \\( S = \\{ S_1, \, \ldots ,\, S_k \\} \\) represents the groupings where \\( S_i \\)
is the set of data points contained in the cluster \\( i \\). The number of samples in \\( S_i \\) is noted \\( n_i \\).

Notations can have superscripts to illustrate the iterative aspect of the algorithm.
For example, \\( C^t = \\{ \textbf{c}^t_1,\,  ... ,\, \textbf{c}^t_k \\} \\) defines the centroids at step \\( t \\).

**Given some data points and the number of clusters,
the goal is to find the optimal centroids which minimize the within-cluster distance,
also called the within-cluster sum squared error (SSE)**:

\\[ C_{best} = \underset{C}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert^2 \\]


The training procedure is as follow:

1. **Initialization**:
Often, there is no good prior knowledge about the location of the centroids.
An effortless way to start is to define the centroids by randomly selecting
\\( k \\) data points from the dataset.
<br/>In mathematical notations, we define \\( C^0 \\), the initial set of centroids,
as a subset of data points with a cardinality of \\( k \\):
\\( C^0 \subset X \\) with \\( \vert C^0 \vert = k \\).

2. **Assignment**:
For each data points, the distance to all centroids is computed.
The data points belong to the cluster represented by the closest centroid.
This is called a **hard-assignment because data point belongs to one and only one cluster**.
<br/>Data points are assigned to partitions as follow:
\\[ S^t_i = \\{\textbf{x} \in X  ;\; \lVert \textbf{x} - \textbf{c}_i \rVert < \lVert \textbf{x} - \textbf{c}_j \rVert, \; \forall 1 \leq j \leq k \\} \\]

3. **Update**:
Given all the points assigned to a cluster, the mean position is computed
and defined the new location of the centroid. All centroids are updated simultaneously.
\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = \frac{1}{\vert S^t_i \vert } \sum\_{\textbf{x} \in S^t_i}{\textbf{x}} \\]


4. **Repeat steps 2 and 3 until convergence**.
The algorithm can stop after a predefined number of iterations.
Another convergence criteria could be to stop whenever
the centroids move less than a certain threshold during step 3.

Once the algorithm has converged and is used on new data, only the assignment step is
performed.


## Visualisation
<hr>
<div>
<div style="text-align: center;">
    Number of clusters \(k\): 
    <input type="number" id="nb-clusters" value="3" min="1" style="width:100px;">
</div>
<div style="text-align: center;margin-bottom: 2rem;">
  <img id="data-0" src="/assets/img/data0.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
  <img id="data-1" src="/assets/img/data1.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
  <img id="data-2" src="/assets/img/data2.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
  <img id="data-3" src="/assets/img/data3.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
  <img id="data-4" src="/assets/img/data4.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
  <img id="data-5" src="/assets/img/data5.png" alt="data 0" style="width:100px;height:100px;box-shadow: 0 3px 6px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);margin-bottom:0"/>
</div>
<div style="text-align: center;">
  <button id="btn-run" class="button__outline">Run 10 iterations</button>
</div>
<div id="container" style="text-align: center;"></div>
</div>
<hr>

<script language="javascript">

// Transform 2-D standard normal data
function sample2DNormal(mu, sig){
    // Perform Cholesky decomposition$primary
    const a = Math.sqrt(sig[0][0]);
    const b = sig[0][1] / a;
    const c = Math.sqrt(sig[1][1] - b * b);
    const sqrtSig = [[a, 0], [b, c]];

    // Get random point
    const stdNorm = d3.randomNormal(0, 1);
    const u = [stdNorm(), stdNorm()];

    // Transform
    const v = {};
    v.x = mu[0] + sqrtSig[0][0] * u[0] + sqrtSig[0][1] * u[1];
    v.y = mu[1] + sqrtSig[1][0] * u[0] + sqrtSig[1][1] * u[1];

    return v;
}

function l2dist(a, b) {
    const dx = b.x - a.x;
    const dy = b.y - a.y;
    return Math.sqrt(Math.pow(dx, 2) + Math.pow(dy, 2));
}

function findClosestCentroid(point, centroids) {
    let closest = {i: -1, distance: 100000};
    centroids.forEach(function(d, i) {
        const distance = l2dist(d, point);
        if (distance < closest.distance) {
            closest.i = i;
            closest.distance = distance;
        }
    });
    return closest.i;
}

const w = 320;
const h = 320;
const pad = 5;
let k = 3;
let iter = -1;

// Define plot scale
const xScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([pad, w-pad]);
const yScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([pad, h-pad]);

const svg = d3.select("#container")
  .append("svg")
  .attr("class", "animation")
  .attr("width", w)
  .attr("height", h)
  .style("text-align", "center");

d3.select("#nb-clusters").attr("value", k).on("input", function() {
    while (this.value < k) {
        centroids.pop();
        k = k - 1;
    }
    if (this.value > k) {
        centroids = centroids.concat(select_centroids(data, this.value - k));
        k = this.value;
    }
    svg.selectAll(".centroid")
        .data(centroids)
        .exit()
        .remove();
    draw_centroids(centroids);
    if (iter == 0) {
        iter = 10;
        start();
    }
})

function draw_data(data) {
    const d = svg.selectAll(".data").data(data)
    d.exit()
     .transition()
     .duration(100)
     .attr("r", 0)
    d.enter()
     .append("circle")
     .merge(d)
     .attr("class", "data")
     .attr("cx", d => d.x)
     .attr("cy", d => d.y)
     .attr("r", 0)
     .transition()
     .duration(150)
     .attr("r", 4)
     .transition()
     .duration(150)
     .attr("r", 3)
}

let data = []

d3.select("#data-0").on("click", function() {
    let points = []
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([-5, -5], [[4, 0], [0, 4]]);
    }));
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([5, 5], [[4, 0], [0, 4]]);
    }));
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([5, -5], [[4, 0], [0, 4]]);
    }));
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([-5, 5], [[4, 0], [0, 4]]);
    }));
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
}).dispatch('click');

//d3.select("#data-1").on("click", function() {
//    let points = []
//    points = points.concat(d3.range(50).map(i => sample2DNormal([-3, -3], [[3, -2], [-2, 3]])));
//    points = points.concat(d3.range(50).map(i => sample2DNormal([ 5, -2], [[2, -1], [-1, 2]])));
//    points = points.concat(d3.range(50).map(i => sample2DNormal([ 0,  3], [[2,  0], [ 0, 2]])));
//    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
//    draw_data(data);
//    if (iter == 0) {
//        iter = 10;
//        start();
//    }
//});
d3.select("#data-1").on("click", function() {
    let points = []
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([-5.5, 0], [[0.2, 0], [0, 7]]);
    }));
    points = points.concat(d3.range(80).map(function() {
        return sample2DNormal([1.5, 0], [[7, 0], [0, 0.2]]);
    }));
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
});

d3.select("#data-2").on("click", function() {
    let points = []
    points = points.concat(d3.range(90).map(function(i) {
        noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
        const r = 7;
        const rad = i * 4 / 180 * Math.PI;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    }));
    points = points.concat(d3.range(20).map(i => sample2DNormal([-2, -1.75], [[0.5, 0], [0, 0.5]])));
    points = points.concat(d3.range(20).map(i => sample2DNormal([2, -1.75], [[0.5, 0], [0, 0.5]])));
    points = points.concat(d3.range(30).map(function(i) {
        noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
        const r = 3.5;
        const rad = i * 4 / 180 * Math.PI + Math.PI / 6;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    }));
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
});

d3.select("#data-3").on("click", function() {
    let points = []
    points = points.concat(d3.range(90).map(function(i) {
        noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
        const r = 7;
        const rad = i * 4 / 180 * Math.PI;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    }));
    points = points.concat(d3.range(45).map(function(i) {
        noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
        const r = 3;
        const rad = i * 8 / 180 * Math.PI;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    }));
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
});


d3.select("#data-4").on("click", function() {
    let points = []
    points = points.concat(d3.range(45).map(function(i) {
        noise = sample2DNormal([-2.75,-1], [[0.15, 0], [0, 0.30]]);
        const r = 6;
        const rad = i * 4 / 180 * Math.PI;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    }));
    points = points.concat(d3.range(45).map(function(i) {
        noise = sample2DNormal([2.75,1], [[0.15, 0], [0, 0.30]]);
        const r = 6;
        const rad = i * 4 / 180 * Math.PI + Math.PI;
        return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
    })); 
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
});


d3.select("#data-5").on("click", function() {
    let points = []
    points = points.concat(d3.range(60).map(function(i) {
        return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
    }).filter(
        d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
    ).map(
        d => ({x: d.x + 5, y: d.y - 4})
    ));
    points = points.concat(d3.range(60).map(function(i) {
        return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
    }).filter(
        d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
    ).map(
        d => ({x: d.x - 5, y: d.y - 4})
    ));
    points = points.concat(d3.range(200).map(function(i) {
        return {x: d3.randomUniform(-5, 5)(), y: d3.randomUniform(-5, 5)() + 1}
    }).filter(
        d => Math.sqrt(d.x * d.x + d.y * d.y) < 5
    ).map(
        d => ({x: d.x, y: d.y + 1})
    ));
    data = points.map(d => ({x: xScale(d.x), y: yScale(d.y)}));
    remove_visual_annotations();
    draw_data(data);
    if (iter == 0) {
        iter = 10;
        start();
    }
});



function draw_centroids(centroids) {
    svg.selectAll(".centroid")
        .data(centroids)
        .enter()
        .append("circle")
        .attr("class", "centroid")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .style("fill", (d, i) => colors(i))
        .style("stroke", "black")
        .style("stroke-width", 2)
        .attr("r", 0)
        .transition()
        .duration(150)
        .attr("r", 8)
        .transition()
        .duration(150)
        .attr("r", 6);
}

function draw_cluster_assignment(data) {
    svg.selectAll(".line")
        .data(data)
        .enter()
        .append("line")
        .attr("class", "line")
        .attr("x1", d => d.x)
        .attr("y1", d => d.y)
        .attr("x2", d => d.x)
        .attr("y2", d => d.y)
        .style("stroke", d => colors(d.cluster))
        .style("stroke-opacity", 0.4)
        .transition()
        .delay(d => Math.random() * 500)
        .duration(300)
        .attr("x2", d => centroids[d.cluster].x)
        .attr("y2", d => centroids[d.cluster].y)

    svg.selectAll(".data")
        .transition()
        .delay(d => Math.random() * 500)
        .duration(100)
        .style("fill", (d, i) => colors(d.cluster));
}

function draw_centroids_update(new_centroids) {
    svg.selectAll(".centroid")
       .data(new_centroids)
       .transition()
       .delay(750)
       .duration(500)
       .attr("cx", d => d.x)
       .attr("cy", d => d.y)

    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(750)
       .duration(500)
       .attr("x2", d => new_centroids[d.cluster].x)
       .attr("y2", d => new_centroids[d.cluster].y)
}

function remove_visual_annotations() {
    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(1500)
       .style("stroke-opacity", 0)
       .remove()

    svg.selectAll(".data")
       .data(data)
       .transition()
       .delay(1500)
       .style("fill", "black")

}

function select_centroids(data, k) {
    return d3.range(k).map(function() {return data[Math.floor(Math.random() * data.length)]});
}

function assign_cluster(data, centroids) {
    data.forEach(d => d.cluster = findClosestCentroid(d, centroids));
}

function compute_centroids(k) {
    centroids = d3.range(k).map(function(i) {
        const points_in_cluster = data.filter(d => d.cluster == i)
        return {
          x: d3.mean(points_in_cluster.map(d => d.x)),
          y: d3.mean(points_in_cluster.map(d => d.y))
        };
    });
    return centroids;
}

var colors = d3.scaleOrdinal().domain(d3.range(k)).range(d3.schemeCategory10);
let centroids = [];
draw_data(data, svg);

play_layer = svg.append("g"); 
play_layer.append("rect")
  .attr("width", "100%")
  .attr("height", "100%")
  .attr("fill", "white")
  .attr("fill-opacity", 0.5)
play_layer.append("path")
  .attr("d", d3.symbol().type(d3.symbolTriangle).size(4000))
  .attr("fill", "black")
  .attr("transform", function(d) { 
    return "rotate(-30) translate(50, 200)";
  });

function animate() {
    const delaunay = d3.Delaunay.from(centroids, d => d.x, d => d.y);
    const voronoi = delaunay.voronoi([0, 0, w, h]);
    let v = svg.selectAll(".voronoi")
        .data(centroids);
    v.enter()
        .append("path")
        .attr("class", "voronoi")
        .attr("d", (d, i) => voronoi.renderCell(i))
        .attr("fill-opacity", 0.25)
        .attr("fill", (d, i) => colors(i))
        .attr("stroke", "white")
        .attr("stroke-width", 0.5);
    v.exit().remove();
    v.transition()
        .duration(1000)
        .attr("d", (d, i) => voronoi.renderCell(i))

    assign_cluster(data, centroids);
    draw_cluster_assignment(data);
    centroids = compute_centroids(k);
    draw_centroids_update(centroids);
    remove_visual_annotations();
    iter = iter - 1;
    if (iter > 0) {
        svg.transition().delay(2000).on("start", animate);
    }
}

function start(){
    iter = 10;
    play_layer.remove()
    if (centroids.length == 0){
        centroids = select_centroids(data, k);
    }
    draw_centroids(centroids, svg);
    animate()
};
svg.on("click", start);
d3.select("#btn-run").on("click", start);
</script>


<br>
It is possible to visualize the decision boundaries.
Do to so, we can generalize the definitions of the groups and include all possible elements rather than
only observed data points:
\\[ S_i = \\{x \in \mathbb{R}^d  ;\; ||x - c_i || < || x - c_j ||, \; \forall 1 \le j \le k \\} \\]
These are called [Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram) and they look like this:
<figure>
  <img src="/assets/img/voronoi_k2.png" alt="Voronoi cells, k=2" style="max-width:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-width:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k5.png" alt="Voronoi cells, k=5" style="max-width:300px;display:inline-block;width:32%;position: center;">
  <figcaption>Fig.2 - Voronoi cells with k=2 (left), k=3 (middle) and k=5 (right).</figcaption>
</figure>

## Python Implementation using Numpy

Although the algorithm could be implemented with plenty of nested for-loops,
it can execute several orders of magnitude faster if we leverage matrix arithmetic.
Numpy {%cite harris2020array -f kmeans %} is an [open-source Python library](https://numpy.org/)
designed to ease the manipulation of vectors, matrix and arrays of any dimension.
The core operations are written in C, a low-level language, to achieve fast runtime.

You can use `conda` or `pip` package managers to install Numpy.
<br/> conda: `conda install numpy`
<br/> pip: `pip install numpy`

Let's start by importing Numpy and creating some synthetic data generated from three normal distributions.
```python
import numpy as np

blob1 = np.random.multivariate_normal(mean=[-3, 3], cov=[[3, 2], [2, 3]], size=100)
blob2 = np.random.multivariate_normal(mean=[5, 2], cov=[[2, 1], [1, 2]], size=100)
blob3 = np.random.multivariate_normal(mean=[0, -3], cov=[[2, 0], [0, 2]], size=100)
data = np.vstack([blob1, blob2, blob3])
```

The next step is to define a function for each step of the algorithm.
```python
def pick_centroids(data, k):
    indexes = np.random.choice(len(data), size=k, replace=False)
    centroids = data[indexes]
    return centroids

def assign_cluster(data, centroids):
    # Pairwise squared L2 distances. Shape [n, k]
    distances = ((data[:, np.newaxis] - centroids)**2).sum(axis=2)
    # find closest centroid index. Shape [n]
    clusters = np.argmin(distances, axis=1)
    return clusters

def update_centroids(data, clusters, k):
    # Mean positions of data within clusters
    centroids = [np.mean(data[clusters == i], axis=0) for i in range(k)]
    return np.array(centroids)
```

The final step is to glue everything together inside a class:
```python
class KMeans:
    def __init__(self, k=3):
        self.k = k
        
    def fit(self, data, steps=20):
        self.centroids = pick_centroids(data, self.k)
        for step in range(steps):
            clusters = assign_cluster(data, self.centroids)
            self.centroids = update_centroids(data, clusters, self.k)
            
    def predict(self, data):
        return assign_cluster(data, self.centroids)
```

Finally, we can instantiate an object, train it and do the predictions.
```python
kmeans = KMeans(k=3)
kmeans.fit(data)
clusters = kmeans.predict(data)
```

If you have Matplotlib installed (`pip install matplotlib`) you can visualize the data and the result.

```python
import matplotlib.pyplot as plt
# Plot data points with cluster id as color
plt.scatter(data[:, 0], data[:, 1], c=clusters)
# Plot centroids
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c="red")
plt.show()
```
You should have something looking like this:

<figure>
  <img src="/assets/img/matplotlib.png" alt="Matplotlib display" style="max-width:350px;display:inline-block;width:90%;">
  <figcaption>Fig.3 - Matplotlib cluster visualization</figcaption>
</figure>

Go ahead and run the code step by step to build a better grasp of Numpy.
Read the [Numpy documentation about broadcasting operations](https://numpy.org/doc/stable/user/basics.broadcasting.html)
to understand how the pairwise distance is computed with a single line of code.

I suggest you do not use this code if it's not for educational purpose. Scikit-learn {%cite scikit-learn -f kmeans %}
is a very popular [open-source Python library](https://scikit-learn.org/stable/), build on top of Numpy,
which implements [the naive k-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=k%20means#sklearn.cluster.KMeans), its variants
and many more machine learning algorithms.


## Limitations and Variants

***K-means* strength is its conceptual simplicity**. The logic but also the implementation
are intuitive and straightforwards. However, it is necessary to understand
that the algorithm was built around restrictive hypothesis about the data
distribution. Some tweaks and variants overcome some of the limitations.

### Sensitivity to Initialization

The first drawback of k-means which can easily be observed after a few runs is the sensitivity
of the algorithm to the initial locations of the centroids.
**For each possible initialization corresponds a certain convergence speed and a final solution**.
Some solutions correspond to local minima and are far from optimal.

A naive way to address this sensitivity to initialization is to run the algorithm several
times and keep the best model. Another solution to get around is to use expert knowledge
to hand-craft what could be a good initialization. A third solution is to **come up with
some heuristics to define a good initialization state. These rules are called seeding techniques**.

<h4 class="no_toc">Seeding techniques</h4>

Many seeding heuristics have been developed over time.
*K-means++* {% cite arthur2006k -f kmeans %} is one of the most popular one
and the default initialization method in Scikit-learn (and Matlab).
The motivation behind is that **spreading out the initial centroids
is beneficial**. Indeed, picking up spread centroids increase chances
that they belong in different clusters.

The *k-means++* seeding algorithm is as follow:
- Randomly select the first set of centroids among the data points.
- For all data points, compute the distance to the closest centroid \\( D(\textbf{x}) \\).
- Pick up another centroid from the set of data points, given the weighted
distribution proportional to the squared
distances: \\( p(\textbf{x}\_i)= \frac{ D(\textbf{x}_i)^2 }{ \sum\_{\textbf{x} \in X} D(\textbf{x})^2 } \\)
- Repeat the two previous steps \\( k-1 \\) times.

The benefits in terms of convergence speed and final error outweigh the computational overhead
for this initialization procedure, see the original paper {% cite arthur2006k -f kmeans %} for details.
If you are looking for a thorough comparison of seeding techniques, I recommend this recent review
{% cite franti2019much -f kmeans %}.

Keep in mind, a good initialization of the centroids should be as close as possible to the
optimal solution. **Thus finding a good initialization is not easier than solving the clustering problem itself.**


### Computational Time Complexity

The assignment step time complexity is \\( O(nkd) \\)
where \\( n \\) is the number of samples, \\( k \\) is the number of clusters
and  \\( d \\) is the input space dimension. **This complexity is the consequence
of computing the pair-wise distance between all data points and all centroids**.
The update step has a time complexity of \\( O(nd) \\). The mean is computed along \\( d \\) dimensions for
\\( k \\) clusters, each containing an average of \\( n/k \\) data points.

The overall time complexity at each step of the Lloyd algorithm is therefore \\( O(nkd) \\).
If all these values increase by two-folds at once, then the algorithm will be eight times slower, not ideal.

<h4 class="no_toc">Dimensionality Reduction</h4>
There are some ways to mitigates this scaling problem. The first one is to retains only the relevant dimensions
with a feature selection. The second one is to **project the data points into a smaller space** with
[principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis),
[independent component analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis)
or [linear discriminant analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis).

<h4 class="no_toc">Batch Processing</h4>
A second approach to reducing the computational complexity is to use fewer
samples at each iteration. **Rather than working
with all data points, the algorithm only manipulates a randomly selected subset.
This variant is called *mini-batch k-means***.
We usually want the batch size, noted \\( b \\), to be far greater than the number of cluster
and much-lower than the number of samples (\\( k \ll b \ll n \\)).


### Sensitivity to Outliers

The centroid locations are defined as the mean positions of the data points within each cluster.
However, **the mean is an estimator sensitive to outliers**. Outliers are data points which are 
significantly different from the other ones (out of distribution samples).
Fig.3 illustrates this issue where two data points out of distribution have a disproportionate impact on
the final result. **Not only some points are not assigned to the right cluster, but the decision
boundary is ultimately different**. 


<figure>
  <img src="/assets/img/outliers_off.png" alt="k-means nwithout outliers" style="max-width:300px;display:inline-block;width:48%;">
  <img src="/assets/img/outliers_on.png" alt="kmeans with outliers" style="max-width:300px;display:inline-block;width:48%;">
  <figcaption>Fig.4 - Without outliers (left) and with outliers (right)</figcaption>
</figure>

If possible, a solution could be to **preprocess the data by filtering out the outliers**.
A different remedy could be to **weight each data points**. During the update step,
weighted-mean is therefore used. *Weighted k-means* can reduce the impact of the outliers
and avoid the need for a hard decision in the preprocessing.
Finding a good weighting strategy is not easy and might take a few trials.


<h4 class="no_toc">Different Aggregators</h4>
There are two more variants of *k-means* which are designed to reduce the influence of outliers.
For the first one, **the centroids are no longer defined as the mean locations but as the median positions.
The algorithm is rightfully called *k-medians***. *K-means* minimize the within cluster-variance (L2-norm)
while *k-medians* minimize the absolute deviation (L1-norm). For k-medians the update rule becomes:

\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = \text{median}( S^t_i ) \\]

The second adaptation enforces some restrictions to centroids. Rather than taking any possible locations, 
the **centroids are restricted to the set of data points. This algorithm is called *k-medoids***
(or PAM for Partitioning Around Medoids). A medoid is the most central representants in a cluster
that minimize the sum of distances to other data points. This distance metric makes *k-medoids*
more robust to outliers.
For *k-medoids*, the optimal solution is given by:

\\[ C_{best} = \underset{X}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert \\]

Finding the right medeoids requires a different algorithm (out of scope for
this post) and has a higher computational complexity of \\( O(k(n-k)^2) \\) at each iteration step.
See PAM, CLARA and CLARANS algorithms {% cite schubert2019faster -f kmeans %} if you are interested.

### Shape and Extend of Clusters

<h4 class="no_toc">Convexity of Partitions</h4>

K-means, and all its variants, minimize distortion and **optimize the clusters for compactness.
The implicit assumption is that clusters are roughly spherical**
([isotropic](https://en.wikipedia.org/wiki/Isotropy), uniform in all directions).
Depending on the distribution of data points, some clusters might not be exactly spherical, but they
will **at least form convex partitions**. Convex means that for any two points in the same cluster,
the whole segment from one point to the other is fully contained inside the Voronoi cell.

Some good examples of failing cases are the concentric circles, the half-moons and the smiley
as illustrated in the figure below. There is no way for *k-means* to isolate
the inner ring from the external ring as the outermost partition will not be convex.
Same problem on the 4 components of the smiley. For the two half-moons, the geometrical
properties of each cluster cannot be captured.

<figure>
  <img src="/assets/img/rings.png" alt="Rings dataset" style="max-width:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/halfmoons.png" alt="Half moons datasets" style="max-width:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/smiley.png" alt="Smiley datasets" style="max-width:300px;display:inline-block;width:32.5%;">
  <figcaption>Fig.5 - Partitions for non-convex and geometric clusters. Concentric&nbsp;rings&nbsp;(left, k=8), half&nbsp;moons&nbsp;(center,&nbsp;k=4) and smiley&nbsp;(right,&nbsp;k=12)&nbsp;datasets</figcaption>
</figure>

One partial solution is to **overestimate the number of clusters and merge the partitions later on**.
However, that might not be easy for high-dimensional data and it will never really compensate if the data
distribution violates the initial assumption of spherical/convex clusters.

One might want to use the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)
to operate in a higher-dimensional space. The hope is that in the projected space the data will comply with the spherical
distribution hypothesis. Finding the right kernel function could be cumbersome.

<h4 class="no_toc">Similar Expansions</h4>
A further limitation is the relative size of clusters. By placing the decision boundaries halfway between 
centroids, **the underlying assumption is that cluster are evenly sized along that direction**.
The size is defined as the spatial extend (area, volume or [Lebesgue measure](https://en.wikipedia.org/wiki/Lebesgue_measure)) not the number of data points assigned inside the cluster.

Once again some datasets with complex geometrical shapes
might not match this assumption. The perfect example is the Mickey Mouse dataset.

<figure>
  <img src="/assets/img/mickey.png" alt="mickey dataset" style="max-width:300px;display:inline-block;width:49%;">
  <img src="/assets/img/tshape.png" alt="T shape datasets" style="max-width:300px;display:inline-block;width:49%;">
  <figcaption>Fig.6 - K-means partitions with anisotropic clusters and different&nbsp;variances.</figcaption>
</figure>

Finally, the *k-means* could fail if data arises from blobs with similar size, but some generates far
fewer observations. It is somewhat related to the initialization problem as selecting a centroid from
the underrepresented class is less likely.

To sum up, k-means clustering assumes the clusters have convex shapes (e.g. a circle, a sphere),
all partition should have similar sizes, and the clusters are balanced.


<h4 class="no_toc">Capturing Variances</h4>
*K-means* and its variants only learn the central location of clusters, nothing more.
**It would be relevant to also capture the spatial extent of each cluster**.
**[Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model)** (*GMM*)
was designed to capture the mean location and standard deviation of the data.

*GMM* computes the probability for all data points to belong to a cluster, for all clusters. 
This is called a **soft-assignment** in opposition to the hard-assignment to one and
only one cluster used by *k-means*. *GMM* is a powerful tool. There is a lot to say about it,
but that will be a story for another time.


### Optimal Number of Clusters

Last but not least, finding the optimal number of cluster is necessary to achieve good performances.
Priors or domain knowledge can be very handy for this task.
To this day, more than 30 methods have been published on this topic.
I will try to cover the most common methods.

#### The Elbow Method

First of all, we need to define a metric which tells how compact the clusters are.
A pertinent metric for this is the **within-cluster sum squared error** (\\(SSE\\) or \\(WSS\\)).
It is defined as the sum of squared distance between each point and its closest centroid.

\\[ SSE(k) = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 \\]

This function is strictly decreasing when applied on converged models. At first,
when we add clusters it gets easier to model the data. But after a certain point,
increasing the number of clusters brings little additional benefits
as it only split clusters into sub-groups or start modelling the noise.
**The *Elbow method* {% cite thorndike1953belongs -f kmeans %} is a heuristic to
find this point of diminishing return**.
The analysis is performed by plotting the evolution of the explained variance as the number 
of clusters grows. **The optimal number of cluster is located at the bend,
the elbow of the curve.**


<figure>
  <img src="/assets/img/elbow.png" alt="Explained variance" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.7 - Explained variance</figcaption>
</figure>

\\( SSE \\) is often applied once the model has converged.
However, it can also be used as a stopping criteria for the algorithm.
If \\( SSE \\) is computed at the end of each iteration, a heuristic
could be to stop the training when the \\( SSE \\) decreases less than a
certain threshold compared to the previous step.


#### Gap Statistic

The *Elbow Method* is far from ideal. The elbow is not formally defined
and a visual inspection is necessary to localise it.
Consequently, the estimation of the optimal number of clusters can be subjective.
Not all SSE curves will have as sharp elbow like in the example above!

The *gap statistic analysis* {%cite tibshirani2001estimating -f kmeans %} address this problem.
The idea behind is to **compare the SSE measured on the data with the SSE computed on a dataset
without obvious clustering** (data with a null reference distribution). The SSE on the data should be 
further away from the reference value for the optimal number of clusters.

The equation looks like this :

\\[ G(k) =  \log SSE(X^*, k) - \log SSE(X, k) \\]

We denote by \\( X^* \\) a dataset with data points **uniformly distributed**
in the interval defined by the minimum and maximum values of \\( X\\) along each \\( d \\) components.

The expected SSE on the null distribution, the first term of the equation,
is **estimated with bootstrapping**. This means, we sample \\( B \\) different datasets
\\(X_b^\* \; \forall 1 \leq b \leq B\\) and compute the mean logarithm of SSE.
The estimation gets more accurate with higher values of B.
However, keep in mind that \\( G_B(k) \\) is only computed once *k-means* has converged \\( B+1 \\) times.
The expression of the gap function becomes:

\\[ G_B(k) = \frac{1}{B} \sum_{b=1}^{B} \log SSE(X_b^*, k) - \log SSE(X, k) \\]

The optimal number of clusters is the smallest k such that \\( G(k) \geq G(k+1) - s_{k+1} \\) where
\\( s_k = \sqrt{1 + 1/B}\;\text{std}_B(\log SSE(X_b^\*, k)) \\) accounts for the simulation error.

Our toy example is simple enough so identifying the k which meet this condition is straight forward (k=4).
<figure>
  <img src="/assets/img/gap.png" alt="kmeans gap statistic analysis" style="max-width:660px;display:inline-block;width:100%;">
  <img src="/assets/img/gap-cond.png" alt="kmeans gap condition" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.10 - Gap statistic analysis</figcaption>
</figure>



#### Silhouette Analysis

Silhouette coefficient {% cite rousseeuw1987silhouettes -f kmeans %} is a
**quality measurement for partitioning**. The silhouette score measures how close
a data point is to its counterparts in the same cluster compared to other elements
in other clusters. **The score captures the cohesion (within-cluster distances)
and the separation (inter-cluster distances)**.

The average dissimilarity of a data point \\( \textbf{x} \\) with elements in a cluster is defined as:

\\[ d(\textbf{x}, S_i) = \frac{1}{n_i - 1} \sum\_{\textbf{x}_i \in S_i} \vert\vert \textbf{x}_i - \textbf{x} \vert\vert \\]

The cohesion score (the lower, the better) of \\( \textbf{x} \\), which belongs to
partition \\( S\_{\textbf{x}} \\), is the intra-cluster
distance: \\( a(\textbf{x}) = d(\textbf{x}, S\_{\textbf{x}}) \\).
The partition which does not contain \\( \textbf{x} \\) and minimize \\( d(\textbf{x}, S_i) \\)
is called the **neighboring cluster**. The separation score (the higher, the better) is
defined as the average dissimilarity of \\( \textbf{x} \\) with the neighbouring
cluster: \\( b(\textbf{x}) = \min\_{i, \textbf{x}\notin S\_i} d(\textbf{x}, S\_i) \\).
The silhouette coefficient combines these two scores:

\\[ s(\textbf{x}) = \frac{b(\textbf{x}) - a(\textbf{x})}{\max\\{a(\textbf{x}), b(\textbf{x})\\}} \\]

If the cluster only contains 1 data point, the silhouette score is set to 0.
The score varies from -1 to 1. A score close to 1 represents data points well assigned to clusters.
If the score is positive, the data point is in average closer to elements
in the cluster it belongs to rather than elements in the neighbouring cluster.
On the contrary, if it is negative, the data point should belong to the neighbouring cluster.

**The mean silhouette score over all data points is a measure of the partitioning quality**.
This quality score can be computed for several values of \\(k\\).
**The best number of cluster yields the highest average silhouette coefficient**.

It is possible to gain much more insights about the quality of partitionning
by plotting the sorted silhouette coefficients per clusters for different number of clusters.
The silhouette analysis looks like this:

<figure>
  <img src="/assets/img/silhouette234.png" alt="Silhouette k=2,3,4" style="max-width:680px;display:inline-block;width:103%;">
  <img src="/assets/img/silhouette567.png" alt="Silhouette k=5,6,7" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.8 - Silhouette diagram analysis</figcaption>
</figure>

For the **optimal number of cluster, the silhouette plots should**:
- **have the same thickness** (same number of observations)
- **have the same profile**
- **intersect with with the average silhouette score**

In Fig.8, we can rule out \\(k=3,5,6,7\\). The graphical analysis is ambivalent between \\(k=2\\)
and \\(k=4\\). However, the latter has a higher average value. The conclusion of the analysis
is that best results are obtained with \\(k=4\\).

The Elbow method has a linear time complexity of \\(O(nd)\\). The silhouette computes all pairwise distances
so time complexity jump to \\(O(n^2d)\\). It's not cheap and in fact it's worst than the Lloyd algorithm
itself on most datasets.

<!---
<script type="text/javascript" src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<div id="silhouette" style="width:400px;height:500px;"></div>
<script>
let n_i = centroids.map((c, idx) => data.filter(d => d.cluster == idx).length)
for (let i=0; i<data.length; i++) {
  data[i].d = centroids.map(c => 0);
  for (let j=0; j<data.length; j++) {
     data[i].d[data[j].cluster] += l2dist(data[i], data[j]) / (n_i[data[j].cluster] - 1);
  }
  const a = data[i].d[data[i].cluster];
  data[i].d.splice(data[i].cluster, 1);
  const b = Math.min(...data[i].d);
  data[i].s = (b - a) / Math.max(b, a);
}

const cumulativeSum = (sum => value => sum += value)(0);
let n_i_cum = n_i.map(cumulativeSum);
n_i_cum.unshift(0);

const average = arr => arr.reduce((sume, el) => sume + el, 0) / arr.length;
let avg_sil = average(data.map(d => d.s))

let traces = centroids
  .map((c, idx) => data.filter(d => d.cluster==idx).map(d => d.s))
  .map((sil, idx) => {return {
    y: d3.range(n_i_cum[idx] + 10*idx, n_i_cum[idx] + 10*idx + sil.length),
    x: sil.sort(),
    mode: 'line',
    fill: 'tozerox',
    xaxis: 'x',
    yaxis: 'y',
  }})
traces.push({y: [0, n_i_cum[k] + 10*k], x: [avg_sil, avg_sil], mode: 'lines', line: {color: "grey"}});
let layout = {
  showlegend: false,
  annotations: [{
    x: avg_sil,
    y: n_i_cum[k] + 10*k,
    xref: 'x',
    yref: 'y',
    text: 'Avg silhouette value',
    font: {family: 'Times New Roman', size: 16},
    showarrow: true,
    arrowhead: 7,
    ax: 0,
    ay: -10,
  }],
  title: {
    text:'Silhouette Histograms',
    font: {family: 'Times New Roman', size: 16},
  },
  xaxis: {
    title: {
      text: 'Silhouette value (k=' + k + ')',
      font: {family: 'Times New Roman', size: 16},
      standoff: 0,
    },
    range: [-0.1, 0.81]
  },
  yaxis: {
    title: {
      text: 'Histogram per clusters',
      font: {family: 'Times New Roman', size: 16},
      standoff: -100,
    },
    ticks: '',
    showticklabels: false,
    showgrid: false,
  },
};
Plotly.newPlot("silhouette", traces, layout);


function sse(data, centroids) {
 return data.map(d => (d.x - centroids[d.cluster].x)**2 + (d.y - centroids[d.cluster].y)**2).reduce((a, b) => a+b)
}

</script>
-->

#### Measures of Information

**A good estimator of the model performance should take into account the quality of the model
but should also penalise for the complexity of the model. Bayesian Information Criterion** (*BIC*)
{% cite schwarz1978estimating -f kmeans %} and **Akaike Information Criterion** (*AIC*)
{% cite akaike1974new -f kmeans %} measure the statistical quality of a model and respect this
parsimony principle (more parameters should not be added without necessity). For both
metrics, the lower the better.

For k-means, the simplest expressions for \\( BIC \\) and \\( AIC \\) are defined as:
\begin{aligned}
 BIC &= \frac{SSE}{\hat\sigma^2} + kd\log(n) \\\\ 
 AIC &= \frac{SSE}{\hat\sigma^2} + 2kd 
\end{aligned}

\\( \hat\sigma^2 \\) is the average intra-cluster variance defined on a model
with a very high number of clusters {% cite friedman2001elements -f kmeans --label section --locator 7.7 %}:
\\( \hat\sigma^2 = \frac{SSE(k')}{k'},\; k'>k \\). It is worth computing \\( \hat\sigma^2 \\)
for several values of \\( k' \\). Small \\( k' \\) penalizes models with high number of clusters
more heavily

<figure>
  <img src="/assets/img/bic-aic.png" alt="kmeans BIC and AIC measure" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.9 - BIC and AIC</figcaption>
</figure>

From the above figure, we can see that curves for \\(k'=16\\) and \\(k'=20\\)
are overlapping, for both \\( BIC \\) and \\(AIC\\) since
the average intra-cluster variances are nearly equal. In both cases, and for both
metrics, the optimal number of cluster is 4.

The story and the conclusion is a bit different for \\( k'=12 \\).
The minimum for \\( BIC \\) is reached for \\( k = 2 \\) but the minimum 
for \\( AIC \\) is obtained with \\( k = 4 \\). **This illustrates that small
\\(k'\\) leads to models with fewer clusters. It also shows that BIC
puts a higher penalty than \\(AIC\\)**.

<details>
  <summary style="background:#eee">This is not the whole story. Click to expand if you like equations!</summary>
  <div style="background:#eee;padding:5px">

<p>
\(BIC\) and \(AIC\) are originally defined as: 
<br/>
\( BIC = p\log(n) - 2 \log\left(L (M; X) \right) \) and \( AIC = 2p - 2 \log\left(L (M; X) \right) \)
<br/>where \( M \) is the model, \( p \) is the number of free-parameters of \( M \),
and \( L (M; X) \) is the model likelihood.
</p>

<p>
With k-means, we can heavily rely on the assumption that data arises
from identical isotropic blobs. Observed data points can be decomposed
as \( \textbf{x} = \mu + e, \; \forall \textbf{x} \in X \) where the \( \mu \)
is one of the centroids, \(\mu \in C \);
and the error follows a multivariate Gaussians, \( e \sim \mathcal{N}(0,\,\sigma^{2}I_d)\).
</p>

<p>
Given a partition of the space, the probability to observe a data point \( \textbf{x} \) is therefore defined by:

\[ P(\textbf{x} \vert S) = \frac{n_i}{n} 
\frac{1}{\sqrt{2 \pi \sigma^2}^{d}} \exp \left( -\frac{\vert \vert \textbf{x} - \textbf{c}_i \vert \vert_2^2}{2\sigma^2} \right) \]
</p>


<p>
This can be injected into the log-likelihood:

\[ \begin{aligned}
\log & L(S; X)  = \log \prod_{\textbf{x} \in X} P(\textbf{x} \vert S) = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \log P(\textbf{x} \vert S)
               \\ &= \sum_{i=1}^{k} \left[ n_i \log\left(\frac{n_i}{n}\right) -  \frac{\sum_{\textbf{x} \in S_i} \vert \vert \textbf{x} - \textbf{c}_i \vert \vert_2^2}{2\sigma^2}\right] - \frac{nd}{2}\log(2\pi\sigma^2)
\end{aligned} \]

If we consider that all <b>clusters are evenly balanced</b> and if we assume <b>\( \sigma^2 \)
to be fixed</b>, than \( BIC \) and \(AIC\) are equal to the previous equation
with an additive constant. Even if \( \sigma^2 \) is considered fixed, it is unknown. Its value is estimated in
a low-bias regime.
</p>

<p>
Now, <b>let's assume \( \sigma^2 \) is can be estimated from the data.</b>
The unbiased estimator of the variance for each cluster gives:

\[ \forall 1\le i \le k, \; \hat\sigma_i^2 = \frac{1}{d(n_i - 1)} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 \]

Combined with the definition of SSE, we end up with:

\[ SSE = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 = \sum_{i=1}^k d(n_i - 1) \hat \sigma_i^2 \]
</p>

<p>
With the underlying assumption that all clusters have the same extent, we have \( \hat\sigma^2 = \hat \sigma_i^2 \).
In conclusion, the unbiased estimation of the variance is: 

\[ \hat\sigma^2 = \frac{1}{d(n-k)}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 = \frac{SSE}{d(n-k)} \]
</p>

Going back to the log-likelihood formula, it can be simplified into:

\[ \begin{aligned}
\log &  L(S; X) =  \sum_{i=1}^{k} \left[ n_i \log\left(\frac{n_i}{n}\right) \right] - \frac{d(n-k)}{2} - \frac{nd}{2}\log(2\pi\sigma^2)
\end{aligned} \]

In the end, knowing that there are \( p=k\times d \) free parameters for k-means, the \( BIC \) and \( AIC \) expressions become:

\[ \begin{aligned}
 BIC & =(2n + dk)\log(n) + d(n-k) + nd\log(2\pi\sigma^2) - 2 \sum_{i=1}^{k} n_i \log\left(n_i\right)
\\ AIC & = 2n\log(n) + d(n+k) + nd\log(2\pi\sigma^2) - 2 \sum_{i=1}^{k} n_i \log\left(n_i\right)
\\ \text{with}&\; \sigma^2 = \frac{SSE}{d(n-k)}
\end{aligned} \]

There is still one limitation with these equations: they consider
the assignment as the true label for classification rather.
The uncertainty in the cluster assignment is taken into account.
I recommend this article {% cite hofmeyr2020degrees -f kmeans %} which
shows that the number of free parameters is underestimated.

</div></details>

<!--#### Clustergram?-->
<!--{%cite schonlau2002clustergram -f kmeans %}-->


## Conclusion

*K-means* high notoriety is certainly due to its simplicity and ease to implement.
However, it has quite a few limitations: the dependence on the initial centroids,
the sensitivity to outliers, the spherical/convex clusters, and the troubles clustering
groups of different density and spatial extent.

It is also necessary to standardize the data to have zero mean and unit standard deviation.
Otherwise, the algorithm might struggle or give more importance to certain features. 


## Interview Questions

Try to answer the following questions to assess your understanding of *k-means* and its variants.
These questions are often asked during interviews for data science positions.

- What is clustering?
- Is clustering a supervised or an unsupervised technique?
- Can you describe the K-means algorithm? 
- What is its complexity? How would you reduce the number of computations?
- What are some termination criteria?
- What is the difference between hard and soft assignments?
- What are some weaknesses of k-means?  
- The algorithm has some assumptions about the data distribution. What are they?
- How to find the optimal number of clusters?
- How comes two different executions of the algorithm produce different results? How to address this problem?
- What kind of preprocessing would you apply to the data before running the algorithm?
- Is the algorithm sensitive to outliers? If yes, how could you mitigate the problem?
- What are the applications of k-means clustering?
