---
layout: post
title:  "All You Should Know about K-Means Clustering Algorithm"
date:   2020-10-10
---
Recently, I had a spike of interest in outlier detection. It led me
to go down the rabbit hole of clustering techniques. I decided to write
a few notes to crystallize my learnings in a Feynman fashion.
For completeness, I provide a high-level understanding, some step-by-step
animations, a few equations for the formalism, and a Python implementation
using Numpy code. Finally, I will cover the limitations and variants of 
K-means.

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/x-mathjax-config">
MathJax = {
  loader: {load: ['[tex]/color']},
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\[', '\\]']],
    packages: {'[+]': ['color']}
  },
  svg: {
    fontCache: 'glo\(( bal',
    linebreak: 
      automatic: true
  }
};
</script>
<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<script type="text/javascript" src="https://d3js.org/d3.v6.min.js"></script>
<style>
.MathJax {
  overflow-x:auto;
}
svg.animation {
  box-shadow: 0 10px 16px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19) !important;
}
figure {
  display: inline-block;
  margin: 0.5rem 0 2rem 0 !important;
}
</style>


**Clustering is the task of grouping similar looking data points into subsets**.
A group of such data points is called a cluster.
Data points within the same cluster should look alike or share similar
properties compared to other elements in other clusters.

**Clustering is a non-supervised learning technique** mainly used for data
exploration and data mining. Non-supervised means that the data does not come
with any label. The consequence is that **clustering algorithms should look for patterns
inside the data** rather than learning a specific mapping from the data to the
label (supervised learning).

The absence of labels explains the abundance of clustering algorithms.
The input data comes in many forms, the definition of a "good cluster" 
is somehow subjective, and there could be a trade-off between performances
and computations. Naturally, the intended use of the clustering
model also influences which algorithms should be hand-picked and which 
hyper-parameters to tune.
 
**K-means is a centroid model** which means that
each cluster is represented by a central point.
"K" represents the number of clusters and "means" is the operation
applied to data points within a cluster to compute the central location. 
Central points are called centroids or prototypes of the clusters.

<figure>
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-height:300px">
  <figcaption>Fig.1 - Illustration of centroid clustering
	<br>Black dots represent data points, coloured dots are centroids
	<br>and coloured areas show cluster expansions.</figcaption>
</figure>

### The K-means Algorithm

From a theoretical perspective finding the right solution is often 
very difficult (NP-hard). Therefore most algorithms are only approximation
of the optimal solution. The k-means algorithm (sometimes referred to as the Lloyd algorithm)
is an **iterative process which refines the solution and 
converges to a local optimum**.


Let's use some mathematical notations to formalize what I just said.

- \\( X = \\{ \textbf{x}_1, \, \ldots ,\; \textbf{x}_n ;\, \textbf{x}_j \in \mathbb{R}^d \\} \\)
denotes the data points of dimensions \\( d \\).
- \\( C = \\{ \textbf{c}_1, \, \ldots ,\; \textbf{c}_k ;\, \textbf{c}_i \in \mathbb{R}^d \\} \\)
is the set of centroids.
- \\( S = \\{ S_1, \, \ldots ,\, S_k \\} \\) represents the groupings where \\( S_i \\)
is the set of data points contained in the cluster \\( i \\).

To show the iterative aspect of the algorithm, notations can have superscripts
with the iteration step. For example \\( C^t = \\{ \textbf{c}^t_1,\,  ... ,\, \textbf{c}^t_k \\} \\)
 defines the centroids at step \\( t \\).

**Given some data points and the number of clusters,
the goal is to find the optimal centroids which minimize the within-cluster distance**:

\\[ C_{best} = \underset{C}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert \\]


The training is as follow:

1. **Initialization**:
Often, there is no good prior knowledge about the location of the centroids.
An effortless way to start is to define \\( k \\) centroids by to randomly selecting
\\( k \\) data points in the dataset.
<br/>In mathematical notations, we define \\( C^0 \\), the initial centroids,
as a subset of data points with a cardinality of k:
\\( C^0 \subset X \\) with \\( \vert C^0 \vert = k \\).

2. **Assignment**:
For each data points, the distance to all centroids is computed.
The data points belong to the cluster represented by the closest centroid.
This is called a hard-assignment because data point belongs to one and only one cluster.
<br/>Data points are assigned to partition as follow:
\\[ S^t_i = \\{\textbf{x} \in X  ;\; \lVert \textbf{x} - \textbf{c}_i \rVert \leq \lVert \textbf{x} - \textbf{c}_j \rVert \; \forall 1 \leq j \leq k \\} \\]

3. **Update**:
Given all the points assigned to a cluster, the mean position is computed
and defined the new location of the centroid. All centroids are updated the same way, simultaneously.
\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = \frac{1}{\vert S^t_i \vert } \sum\_{\textbf{x} \in S^t_i}{\textbf{x}} \\]


4. **Repeat steps 3 and 4 until convergence**.
The algorithm can stop after a predefined number of iterations.
Another convergence criteria could be to stop whenever
the centroids move less than a certain threshold during step 3.

Once the algorithm has converged and his presented with new data, only the assignment step is
performed.


### Visualisation


Let's illustrate the algorithm for 2D data points ( \\( d=2 \\) ).
<div id="container" style="display: block;margin: auto;">
    <input type="number" id="nb-clusters" value="3" min="1">
</div>

<script language="javascript">

// Transform 2-D standard normal data
function sample2DNormal(mu, sig){
    // Perform Cholesky decomposition
    const a = Math.sqrt(sig[0][0]);
    const b = sig[0][1] / a;
    const c = Math.sqrt(sig[1][1] - b * b);
    const sqrtSig = [[a, 0], [b, c]];

    // Get random point
    const stdNorm = d3.randomNormal(0, 1);
    const u = [stdNorm(), stdNorm()];

    // Transform
    const v = {};
    v.x = mu[0] + sqrtSig[0][0] * u[0] + sqrtSig[0][1] * u[1];
    v.y = mu[1] + sqrtSig[1][0] * u[0] + sqrtSig[1][1] * u[1];

    return v;
}

function l2dist(a, b) {
    const dx = b.x - a.x;
    const dy = b.y - a.y;
    return Math.sqrt(Math.pow(dx, 2) + Math.pow(dy, 2));
}

function findClosestCentroid(point, centroids) {
    let closest = {i: -1, distance: 100000};
    centroids.forEach(function(d, i) {
        const distance = l2dist(d, point);
        if (distance < closest.distance) {
            closest.i = i;
            closest.distance = distance;
        }
    });
    return closest.i;
}

const w = 300;
const h = 300;
// Define plot scale
const xScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([0, w]);
const yScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([0, h]);

const svg = d3.select("#container")
  .append("svg")
  .attr("class", "animation")
  .attr("width", w)
  .attr("height", h);

const n = 50;
let k = 3;
d3.select("#nb-clusters").attr("value", k).on("input", function() {
    while (this.value < k) {
        centroids.pop()
        k = k - 1;
    }
    if (this.value > k) {
        centroids = centroids.concat(select_centroids(data, this.value - k))
        k = this.value
    }
    svg.selectAll(".centroid")
        .data(centroids)
        .exit()
        .remove()
    draw_centroids(centroids)
    console.log(centroids)
})


// Generate random data
const mu1 = [-3, -3];
const sig1 = [[3, -2], [-2, 3]];
const mu2 = [5, -2];
const sig2 = [[2, -1], [-1, 2]];
const mu3 = [0, 3];
const sig3 = [[2, 0], [0, 2]];

let data = []
data = data.concat(d3.range(n).map(function() {
    return sample2DNormal(mu1, sig1);
}));
data = data.concat(d3.range(n).map(function() {
    return sample2DNormal(mu2, sig2);
}));
data = data.concat(d3.range(n).map(function() {
    return sample2DNormal(mu3, sig3);
}));

//const mu1 = [3, -5];
//const sig1 = [[0.8, 0], [0, 3]];
//const mu2 = [7, -4];
//const sig2 = [[0.8, 0], [0, 3]];
//const mu3 = [-8, 4];
//const sig3 = [[1, 0], [0, 4]];

//let data = []
//data = data.concat(d3.range(90).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 7;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(20).map(function() {
//    return sample2DNormal([-2, -1.75], [[0.5, 0], [0, 0.5]]);
//}));
//data = data.concat(d3.range(20).map(function() {
//    return sample2DNormal([2, -1.75], [[0.5, 0], [0, 0.5]]);
//}));
//data = data.concat(d3.range(30).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 3.5;
//    const rad = i * 4 / 180 * Math.PI + Math.PI / 6;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));

//let data = []
//data = data.concat(d3.range(90).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 7;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 3;
//    const rad = i * 8 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));

//let data = []
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([-2.75,-1], [[0.15, 0], [0, 0.30]]);
//    const r = 6;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([2.75,1], [[0.15, 0], [0, 0.30]]);
//    const r = 6;
//    const rad = i * 4 / 180 * Math.PI + Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));


//let data = []
//data = data.concat(d3.range(60).map(function(i) {
//    return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
//).map(
//  d => ({x: d.x + 5, y: d.y - 4})
//));
//data = data.concat(d3.range(60).map(function(i) {
//    return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
//).map(
//  d => ({x: d.x - 5, y: d.y - 4})
//));
//data = data.concat(d3.range(200).map(function(i) {
//    return {x: d3.randomUniform(-5, 5)(), y: d3.randomUniform(-5, 5)() + 1}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 5
//).map(
//  d => ({x: d.x, y: d.y + 1})
//));

//let data = []
//data = data.concat(d3.range(80).map(function() {
//    return sample2DNormal([-5.5, 0], [[0.2, 0], [0, 7]]);
//}));
//data = data.concat(d3.range(80).map(function() {
//    return sample2DNormal([1.5, 0], [[7, 0], [0, 0.2]]);
//}));

data = data.map(d => ({x: xScale(d.x), y: yScale(d.y)}));

function draw_data(data) {
    svg.selectAll(".data")
        .data(data)
        .enter()
        .append("circle")
        .attr("class", "data")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .attr("r", 0)
        .transition()
        .duration(150)
        .attr("r", 4)
        .transition()
        .duration(150)
        .attr("r", 3)
}

function draw_centroids(centroids) {
    svg.selectAll(".centroid")
        .data(centroids)
        .enter()
        .append("circle")
        .attr("class", "centroid")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .style("fill", (d, i) => colors(i))
        .style("stroke", "black")
        .style("stroke-width", 2)
        .attr("r", 0)
        .transition()
        .duration(150)
        .attr("r", 8)
        .transition()
        .duration(150)
        .attr("r", 6);
}

function draw_cluster_assignment(data) {
    svg.selectAll(".line")
        .data(data)
        .enter()
        .append("line")
        .attr("class", "line")
        .attr("x1", d => d.x)
        .attr("y1", d => d.y)
        .attr("x2", d => d.x)
        .attr("y2", d => d.y)
        .style("stroke", d => colors(d.cluster))
        .style("stroke-opacity", 0.4)
        .transition()
        .delay(d => Math.random() * 500)
        .duration(300)
        .attr("x2", d => centroids[d.cluster].x)
        .attr("y2", d => centroids[d.cluster].y)

    svg.selectAll(".data")
        .transition()
        .delay(d => Math.random() * 500)
        .duration(100)
        .style("fill", (d, i) => colors(d.cluster));
}

function draw_centroids_update(new_centroids) {
    svg.selectAll(".centroid")
       .data(new_centroids)
       .transition()
       .delay(750)
       .duration(500)
       .attr("cx", d => d.x)
       .attr("cy", d => d.y)

    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(750)
       .duration(500)
       .attr("x2", d => new_centroids[d.cluster].x)
       .attr("y2", d => new_centroids[d.cluster].y)
}

function remove_visual_annotations() {
    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(1500)
       .style("stroke-opacity", 0)
       .remove()

    svg.selectAll(".data")
       .data(data)
       .transition()
       .delay(1500)
       .style("fill", "black")

}

function select_centroids(data, k) {
    return d3.range(k).map(function() {return data[Math.floor(Math.random() * data.length)]});
}

function assign_cluster(data, centroids) {
    data.forEach(d => d.cluster = findClosestCentroid(d, centroids));
}

function compute_centroids(k) {
    centroids = d3.range(k).map(function(i) {
        const points_in_cluster = data.filter(d => d.cluster == i)
        return {
          x: d3.mean(points_in_cluster.map(d => d.x)),
          y: d3.mean(points_in_cluster.map(d => d.y))
        };
    });
    return centroids;
}

var colors = d3.scaleOrdinal().domain(d3.range(k)).range(d3.schemeCategory10);
let centroids = [];
draw_data(data, svg);

play_layer = svg.append("g"); 
play_layer.append("rect")
  .attr("width", "100%")
  .attr("height", "100%")
  .attr("fill", "white")
  .attr("fill-opacity", 0.5)
play_layer.append("path")
  .attr("d", d3.symbol().type(d3.symbolTriangle).size(4000))
  .attr("fill", "black")
  .attr("transform", function(d) { 
    return "rotate(-30) translate(50, 200)";
  });

function animate() {
    const delaunay = d3.Delaunay.from(centroids, d => d.x, d => d.y);
    const voronoi = delaunay.voronoi([0, 0, w, h]);
    let v = svg.selectAll(".voronoi")
        .data(centroids);
    v.enter()
        .append("path")
        .attr("class", "voronoi")
        .attr("d", (d, i) => voronoi.renderCell(i))
        .attr("fill-opacity", 0.25)
        .attr("fill", (d, i) => colors(i))
        .attr("stroke", "white")
        .attr("stroke-width", 0.5);
    v.exit().remove();
    v.transition()
        .duration(1000)
        .attr("d", (d, i) => voronoi.renderCell(i))

    assign_cluster(data, centroids);
    //draw_cluster_assignment(data);
    centroids = compute_centroids(k);
    draw_centroids_update(centroids);
    remove_visual_annotations();
    svg.transition().delay(2000).on("start", animate);
}

svg.on("click", function repeat(){
    play_layer.remove()
    centroids = select_centroids(data, k);
    draw_centroids(centroids, svg);
    animate()
});
</script>


<br>
Once the algorithm has converged it is possible to visualize the partitioning
of the input space. For this, we can generalize how we define the k
groups and include all possible points rather
than only the data points:
\\[ S_i = \\{x \in R^d  ;\; ||x - c_i || \le || x - c_j ||, \; \forall 1 \le j \le k \\} \\]
This is called Voronoi cells and looks like this:
<figure>
  <img src="/assets/img/voronoi_k2.png" alt="Voronoi cells, k=2" style="max-height:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-height:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k5.png" alt="Voronoi cells, k=5" style="max-height:300px;display:inline-block;width:32%;position: center;">
  <figcaption>Fig.2 - Voronoi cells with k=2 (left), k=3 (middle) and k=5 (right).</figcaption>
</figure>

### K-means Implementation using Numpy

The algorithm could be implemented with a bunch of for loops. However, the algorithm can
be several orders of magnitude faster if we leverage vector and matrix operations.
[Numpy](https://numpy.org/) is a Python library designed to ease the manipulation of 
vectors, matrix and arrays of any dimension.
The core operations are written in C, a low-level language, to be as fast as possible.

You can use `conda` or `pip` package managers to install Numpy.
<br/> conda: `conda install numpy`
<br/> pip: `pip install numpy`

Let's start by importing Numpy and creating some synthetic data generated from 3 Gaussians.
```python
import numpy as np

blob1 = np.random.multivariate_normal(mean=[-3, 3], cov=[[3, 2], [2, 3]], size=100)
blob2 = np.random.multivariate_normal(mean=[5, 2], cov=[[2, 1], [1, 2]], size=100)
blob3 = np.random.multivariate_normal(mean=[0, -3], cov=[[2, 0], [0, 2]], size=100)
data = np.vstack([blob1, blob2, blob3])
```

The next step is to define a function for each step of the algorithm.
```python
def pick_centroids(data, k):
    indexes = np.random.choice(len(data), size=k, replace=False)
    centroids = data[indexes]
    return centroids
```


```python
def assign_cluster(data, centroids):
    # Pairwise squared L2 distances. Shape: [N, k]
    distances = ((data[:, np.newaxis] - centroids)**2).sum(axis=2)
    # find closest centroid index. Shape [N]
    clusters = np.argmin(distances, axis=1)
    return clusters
```


```python
def update_centroids(data, clusters, k):
    # Mean positions of data within clusters
    centroids = [np.mean(data[clusters == i], axis=0) for i in range(k)]
    return np.array(centroids)
```

The final step is to glue everything together. For that, I use the following class:
```python
class KMeans:
    def __init__(self, k=3):
        self.k = k
        
    def fit(self, data, T=20):
        self.centroids = pick_centroids(data, self.k)
        for t in range(T):
            clusters = assign_cluster(data, self.centroids)
            self.centroids = update_centroids(data, clusters, self.k)
            
    def predict(self, data):
        return assign_cluster(data, self.centroids)
```

Finally, we can instantiate an object, train it and do the predictions.
```python
kmeans = KMeans(k=3)
kmeans.fit(data)
clusters = kmeans.predict(data)
```

If you have Matplotlib installed (`pip install matplotlib`) you can visualize the data and the result.

```python
import matplotlib.pyplot as plt
plt.scatter(data[:, 0], data[:, 1], c=clusters)
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c="red")
plt.show()
```
You should have something looking like this:

![Matplotlib result](/assets/img/matplotlib.png)

I recommend you to run the code step by step to build a better grasp of Numpy.
Read the [Numpy documentation about broadcasting operations](https://numpy.org/doc/stable/user/basics.broadcasting.html)
to understand how the pairwise distance is computed with a single line of code.

I suggest you do not use this code if it's not for educational purpose. [Scikit-learn](https://scikit-learn.org/stable/)
is a very popular open-source Python library, build on top of Numpy,
which implements [the naive k-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=k%20means#sklearn.cluster.KMeans), its variants
and many more machine learning algorithms. 

### Limitations and Variants

K-means strength is its simplicity. The logic but also the implementation
are straightforwards. However, it is necessary to understand what
k-means assumes from the data distribution and when the algorithm fails.

#### Initialization



unstable + permutation


#### Number of Clusters

Finding the right number of clusters could be a bit tricky as it often requires
numerous tweakings. This is where some business insight
might be handy if there is any prior knowledge. Also, trying several values
and waiting for the convergence could be time-consuming.

##### The Elbow Method

The goal of this method is to measure how compact the clusters are after convergence.
A pertinent metric for this is the **with-in cluster sum squared error (SSE)**.
This is defined as the squared distance between each point and its centroid.
It is also called the explained variance.

\\[ SSE(k) = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 \\]


This function should be strictly decreasing. As more clusters are added,
data points get closer to their centroids. Therefore, we are looking
for the point of diminishing return where using more clusters comes with little
additional benefices.

##### Silhouette

##### Measures of information

BIC+AIC

##### Clustergram ?

#### Computational Complexity

The assignment step computational complexity is \\( O(nkd) \\)
where \\( n \\) is the number of samples, \\( k \\) is the number of clusters
and  \\( d \\) is the input space dimension. **This complexity is the consequence
of computing the pair-wise distance between all data points and all centroids**.
The update step has a complexity of \\( O(nd) \\). The mean is computed along \\( d \\) dimensions for
\\( k \\) clusters, each containing an average of \\( n/k \\) data points.

The overall complexity at each step is therefore \\( O(nkd) \\).
This complexity means that the algorithm will be roughly twice longer
if we have twice as many samples, or twice as many clusters, or 
an input space with twice the number of dimension. If all
these values increase by two-folds at once, then the algorithm will be eight times slower.

There are some ways to mitigates this scaling problem.
The first one is to **reduce the number of dimensions by projection
the data points into a smaller space** with principal components
analysis (PCA) or independent component analysis (IDA).

Another approach to reducing the computational complexity is to use fewer
samples at each iteration. **Rather than working
with all data points, the algorithm only manipulates a subset
which randomly selected before each iteration.
This is called *mini-batch K-means***. Choosing the right
number of data point (batch size noted \\( b \\)) could be a bit tricky.
We usually want the batch size to be far greater than the number of cluster
and much-lower than the number of samples (\\( k \ll b \ll n \\)).


#### Sensitivity to Outliers

The centroids locations are defined as the mean position of the data points within each cluster.
However, **the mean is an estimator sensitive to outliers**. Outliers are data points which are 
significantly different from the other ones (out of distribution samples).
Fig.3 shows an example where 2 data points out of distribution have a disproportionate impact on
the final result. **Not only some points are not assigned to the right cluster, but the decision
boundary is ultimately different**. 


<figure>
  <img src="/assets/img/outliers_off.png" alt="Voronoi no outliers" style="max-height:300px;display:inline-block;width:48%;">
  <img src="/assets/img/outliers_on.png" alt="Voronoi with outliers" style="max-height:300px;display:inline-block;width:48%;">
  <figcaption>Fig.3 - Without outliers (left) and with outliers (right)</figcaption>
</figure>

When the algorithm converges, it could be possible that one cluster deals with the outliers
and the other centroids model everything else. This would not be an efficient use of the clusters.
That being said, the benefit is that other centroids are no longer affected by outliers.

If possible, a solution could be to **preprocess the data by filtering out the outliers**.
Another solution could be to **weight each data points**. During the update step,
weighted-mean is therefore used. **Weighted k-means** can reduce the impact of the outliers 
and avoid the need for a hard decision in the preprocessing.
Finding a good weighting strategy is not easy and might take a few trials.

There are two variants of k-means which are designed to reduce the influence of outliers.
For the first variant, **the centroids are no longer defined as the mean locations but as the median position.
The algorithm is rightfully called k-medians**. K-means is an approximation for the optimal clustering problem
if distances are measured with the Euclidean distance (L2) while **k-medians** approximates the solution if
distances are computed with the Manhattan distance (L1). For k-medians the update rule becomes:

\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = median( S^t_i ) \\]

The second variant enforces some restrictions to centroids. Rather than taking any possible locations, 
the **centroids are restricted to be data points. This algorithm is called k-medoids**
(or PAM for Partitioning Around Medoids). A medoid
is the most central representants in a cluster. The optimal solution is given by:

\\[ C_{best} = \underset{X}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert \\]

This algorithm works well for all distance metrics. However, for k-medoids all data points
are potential medoids. Finding the right ones requires a different algorithm (out of scope for
this post) and has a higher computational complexity of \\( O(k(n-k)^2) \\) at each iteration step.
See PAM, CLARA, CLARANS if you are interested.

#### Shape and Extend of Clusters

K-means, and all its variants, optimize the clusters for compactness. **The implicit assumption is that
clusters are roughly spherical**. Depending on the distribution of data points some clusters might not
be spherical but they will **at least be convex partitions**. Convex means that for any two points
in the same cluster, the whole segment line from one point to the other is fully contained inside the
Voronoi cell.

Some good examples are the concentric circles, the half-moons and the smiley data distributions
illustrated in the figure below.
K-means fails to identify the two rings and the two arcs. Same problems on the 4
components of the smiley. One partial solution is to overestimate the number of clusters.
However, that will never compensate if the data distribution does not comply with the initial
assumption of spherical/convex clusters.

<figure>
  <img src="/assets/img/rings.png" alt="Rings dataset" style="max-height:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/halfmoons.png" alt="Half moons datasets" style="max-height:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/smiley.png" alt="Smiley datasets" style="max-height:300px;display:inline-block;width:32.5%;">
  <figcaption>Fig.4 - Partitions for concentric&nbsp;rings&nbsp;(left, k=8), half&nbsp;moons&nbsp;(center,&nbsp;k=4) and smiley&nbsp;(right,&nbsp;k=12)&nbsp;datasets</figcaption>
</figure>

Another limitation is the relative size of clusters. By placing the decision boundaries halfway between 
centroids, **the underlying assumption is that cluster should have similar size along this line**.
The size is defined as the spatial extend (area, volume or [Lebesgue measure](https://en.wikipedia.org/wiki/Lebesgue_measure)) not the number of data points assigned inside the cluster.

Once again it's possible that some datasets with complex geometrical shapes
which do not match this assumption.
The perfect example if the Mickey Mouse dataset.

<figure>
  <img src="/assets/img/mickey.png" alt="mickey dataset" style="max-height:300px;display:inline-block;width:49%;">
  <img src="/assets/img/tshape.png" alt="T shape datasets" style="max-height:300px;display:inline-block;width:49%;">
  <figcaption>Fig.4 - K-means partitions with clusters of different&nbsp;sizes.</figcaption>
</figure>

K-means, and its variances, only learns the central location of clusters and nothing more.
**It would be relevant to also learn the spatial dimensions in each direction**.
Another algorithm, called **Gaussian Mixture Model** (GMM), was designed for that.
It learns both the central locations and shape of clusters. **One other benefit of
this algorithm is the soft-assignment**. Unlike the hard-assignment into one cluster for k-means, GMM
computes a probability to belong to each cluster. GMM is a powerful tool and there is a lot to
say about it, but that will be a story for another time.



