---
layout: post
title:  "All (And More) About K-Means Clustering"
date:   2020-10-10
references: kmeans.bib
---
Recently, I had a spike of interest in outlier detection and
clustering techniques. I went down this rabbit hole so you
don't have to. First stop: *k-means* clustering!
This blog post provides a high-level understanding, some step-by-step
animations, a few equations for math lovers, and a Python implementation
using Numpy. Finally, I will cover the limitations and variants of *k-means*.

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/x-mathjax-config">
MathJax = {
  loader: {load: ['[tex]/color']},
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\[', '\\]']],
    packages: {'[+]': ['color']}
  },
  svg: {
    fontCache: 'glo\(( bal',
    linebreak: 
      automatic: true
  }
};
</script>
<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<script type="text/javascript" src="https://d3js.org/d3.v6.min.js"></script>
<style>
.MathJax {
  overflow-x:auto;
}
svg.animation {
  box-shadow: 0 10px 16px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19) !important;
}
figure {
  text-align: center;
  margin: 0.5rem auto 2rem auto !important;
}
</style>

## Introduction

**Clustering is the task of grouping similar looking data points into subsets**.
Such groups of data points are called a clusters.
Data points within the same cluster should look alike or share similar
properties compared to elements in other clusters.

**Clustering is a [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) technique**
mainly used for data exploration and data mining.
*Unsupervised* means that the data does not come with any annotation.
The consequence is that **clustering algorithms should look for patterns
inside the data** (rather than learning a specific mapping from the data to the
label in a [supervised way](https://en.wikipedia.org/wiki/Supervised_learning)).

The absence of labels explains the abundance of clustering techniques.
Each algorithm comes with differente belief about the data
and the definitions of a "good cluster" is somehow subjective.
Naturally, the intended use of the clustering
model also influences which algorithms should be hand-picked and which 
hyper-parameters to tune.

***K-means* is a centroid model** which means
each cluster is represented by a central point.
"K" represents the number of clusters and "mean" is the operation
applied to elements in a cluster to define the central location. 
Central points are called centroids or prototypes of the clusters.

<figure>
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-width:300px">
  <figcaption>Fig.1 - Illustration of centroid clustering
	<br>Black dots represent data points, coloured dots are centroids
	<br>and coloured areas show cluster expansions.</figcaption>
</figure>

## The *K-Means* Algorithm

From a theoretical perspective finding the optimal solution for a clustering problem
is often very challenging (NP-hard). Therefore, all algorithms are only approximations
of the optimal solution. The *k-means* algorithm, sometimes referred to as the Lloyd
algorithm after his author {% cite lloyd1982least -f kmeans %},
is an **iterative process which refines the solution until it
converges to a local optimum**.


Let's use some mathematical notations to formalize what I just said.
- \\( X = \\{ \textbf{x}_1, \, \ldots ,\; \textbf{x}_n ;\, \textbf{x}_j \in \mathbb{R}^d \\} \\)
denotes the set of data points of dimensions \\( d \\).
- \\( C = \\{ \textbf{c}_1, \, \ldots ,\; \textbf{c}_k ;\, \textbf{c}_i \in \mathbb{R}^d \\} \\)
is the set of centroids.
- \\( S = \\{ S_1, \, \ldots ,\, S_k \\} \\) represents the groupings where \\( S_i \\)
is the set of data points contained in the cluster \\( i \\). The number of samples in \\( S_i \\) is noted \\( n_i \\).

To show the iterative aspect of the algorithm, notations can have superscripts
referring to the iteration step. For example \\( C^t = \\{ \textbf{c}^t_1,\,  ... ,\, \textbf{c}^t_k \\} \\)
 defines the centroids at step \\( t \\).

**Given some data points and the number of clusters,
the goal is to find the optimal centroids which minimize the within-cluster distance**:

\\[ C_{best} = \underset{C}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert^2 \\]


The training procedure is as follow:

1. **Initialization**:
Often, there is no good prior knowledge about the location of the centroids.
An effortless way to start is to define \\( k \\) centroids by randomly selecting
\\( k \\) data points in the dataset.
<br/>In mathematical notations, we define \\( C^0 \\), the initial centroids,
as a subset of data points with a cardinality of \\( k \\):
\\( C^0 \subset X \\) with \\( \vert C^0 \vert = k \\).

2. **Assignment**:
For each data points, the distance to all centroids is computed.
The data points belong to the cluster represented by the closest centroid.
This is called a **hard-assignment because data point belongs to one and only one cluster**.
<br/>Data points are assigned to partition as follow:
\\[ S^t_i = \\{\textbf{x} \in X  ;\; \lVert \textbf{x} - \textbf{c}_i \rVert \leq \lVert \textbf{x} - \textbf{c}_j \rVert \; \forall 1 \leq j \leq k \\} \\]

3. **Update**:
Given all the points assigned to a cluster, the mean position is computed
and defined the new location of the centroid. All centroids are updated simultaneously.
\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = \frac{1}{\vert S^t_i \vert } \sum\_{\textbf{x} \in S^t_i}{\textbf{x}} \\]


4. **Repeat steps 2 and 3 until convergence**.
The algorithm can stop after a predefined number of iterations.
Another convergence criteria could be to stop whenever
the centroids move less than a certain threshold during step 3.

Once the algorithm has converged and is presented with new data, only the assignment step is
performed.


## Visualisation


Let's illustrate the *k-means* algorithm for 2D data points ( \\( d=2 \\) ).
<div id="container" style="display: block;margin: auto;">
    <input type="number" id="nb-clusters" value="3" min="1">
</div>

<script language="javascript">

// Transform 2-D standard normal data
function sample2DNormal(mu, sig){
    // Perform Cholesky decomposition
    const a = Math.sqrt(sig[0][0]);
    const b = sig[0][1] / a;
    const c = Math.sqrt(sig[1][1] - b * b);
    const sqrtSig = [[a, 0], [b, c]];

    // Get random point
    const stdNorm = d3.randomNormal(0, 1);
    const u = [stdNorm(), stdNorm()];

    // Transform
    const v = {};
    v.x = mu[0] + sqrtSig[0][0] * u[0] + sqrtSig[0][1] * u[1];
    v.y = mu[1] + sqrtSig[1][0] * u[0] + sqrtSig[1][1] * u[1];

    return v;
}

function l2dist(a, b) {
    const dx = b.x - a.x;
    const dy = b.y - a.y;
    return Math.sqrt(Math.pow(dx, 2) + Math.pow(dy, 2));
}

function findClosestCentroid(point, centroids) {
    let closest = {i: -1, distance: 100000};
    centroids.forEach(function(d, i) {
        const distance = l2dist(d, point);
        if (distance < closest.distance) {
            closest.i = i;
            closest.distance = distance;
        }
    });
    return closest.i;
}

const w = 300;
const h = 300;
// Define plot scale
const xScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([0, w]);
const yScale = d3.scaleLinear()
    .domain([-10, 10])
    .range([0, h]);

const svg = d3.select("#container")
  .append("svg")
  .attr("class", "animation")
  .attr("width", w)
  .attr("height", h);

const n = 50;
let k = 3;
d3.select("#nb-clusters").attr("value", k).on("input", function() {
    while (this.value < k) {
        centroids.pop()
        k = k - 1;
    }
    if (this.value > k) {
        centroids = centroids.concat(select_centroids(data, this.value - k))
        k = this.value
    }
    svg.selectAll(".centroid")
        .data(centroids)
        .exit()
        .remove()
    draw_centroids(centroids)
})


// Generate random data
const mu1 = [-3, -3];
const sig1 = [[3, -2], [-2, 3]];
const mu2 = [5, -2];
const sig2 = [[2, -1], [-1, 2]];
const mu3 = [0, 3];
const sig3 = [[2, 0], [0, 2]];

//let data = []
//data = data.concat(d3.range(n).map(function() {
//    return sample2DNormal(mu1, sig1);
//}));
//data = data.concat(d3.range(n).map(function() {
//    return sample2DNormal(mu2, sig2);
//}));
//data = data.concat(d3.range(n).map(function() {
//    return sample2DNormal(mu3, sig3);
//}));

//const mu1 = [3, -5];
//const sig1 = [[0.8, 0], [0, 3]];
//const mu2 = [7, -4];
//const sig2 = [[0.8, 0], [0, 3]];
//const mu3 = [-8, 4];
//const sig3 = [[1, 0], [0, 4]];

//let data = []
//data = data.concat(d3.range(90).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 7;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(20).map(function() {
//    return sample2DNormal([-2, -1.75], [[0.5, 0], [0, 0.5]]);
//}));
//data = data.concat(d3.range(20).map(function() {
//    return sample2DNormal([2, -1.75], [[0.5, 0], [0, 0.5]]);
//}));
//data = data.concat(d3.range(30).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 3.5;
//    const rad = i * 4 / 180 * Math.PI + Math.PI / 6;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));

//let data = []
//data = data.concat(d3.range(90).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 7;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([0,0], [[0.15, 0], [0, 0.15]]);
//    const r = 3;
//    const rad = i * 8 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));

//let data = []
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([-2.75,-1], [[0.15, 0], [0, 0.30]]);
//    const r = 6;
//    const rad = i * 4 / 180 * Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));
//data = data.concat(d3.range(45).map(function(i) {
//    noise = sample2DNormal([2.75,1], [[0.15, 0], [0, 0.30]]);
//    const r = 6;
//    const rad = i * 4 / 180 * Math.PI + Math.PI;
//    return { x: r * Math.cos(rad) + noise.x, y : r * Math.sin(rad) + noise.y} 
//}));


//let data = []
//data = data.concat(d3.range(60).map(function(i) {
//    return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
//).map(
//  d => ({x: d.x + 5, y: d.y - 4})
//));
//data = data.concat(d3.range(60).map(function(i) {
//    return {x: d3.randomUniform(-2, 2)(), y: d3.randomUniform(-2, 2)()}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 2
//).map(
//  d => ({x: d.x - 5, y: d.y - 4})
//));
//data = data.concat(d3.range(200).map(function(i) {
//    return {x: d3.randomUniform(-5, 5)(), y: d3.randomUniform(-5, 5)() + 1}
//}).filter(
//    d => Math.sqrt(d.x * d.x + d.y * d.y) < 5
//).map(
//  d => ({x: d.x, y: d.y + 1})
//));

//let data = []
//data = data.concat(d3.range(80).map(function() {
//    return sample2DNormal([-5.5, 0], [[0.2, 0], [0, 7]]);
//}));
//data = data.concat(d3.range(80).map(function() {
//    return sample2DNormal([1.5, 0], [[7, 0], [0, 0.2]]);
//}));

let data = []
data = data.concat(d3.range(80).map(function() {
    return sample2DNormal([-5, -5], [[4, 0], [0, 4]]);
}));
data = data.concat(d3.range(80).map(function() {
    return sample2DNormal([5, 5], [[4, 0], [0, 4]]);
}));
data = data.concat(d3.range(80).map(function() {
    return sample2DNormal([5, -5], [[4, 0], [0, 4]]);
}));
data = data.concat(d3.range(80).map(function() {
    return sample2DNormal([-5, 5], [[4, 0], [0, 4]]);
}));

data = data.map(d => ({x: xScale(d.x), y: yScale(d.y)}));

function draw_data(data) {
    svg.selectAll(".data")
        .data(data)
        .enter()
        .append("circle")
        .attr("class", "data")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .attr("r", 0)
        .transition()
        .duration(150)
        .attr("r", 4)
        .transition()
        .duration(150)
        .attr("r", 3)
}

function draw_centroids(centroids) {
    svg.selectAll(".centroid")
        .data(centroids)
        .enter()
        .append("circle")
        .attr("class", "centroid")
        .attr("cx", d => d.x)
        .attr("cy", d => d.y)
        .style("fill", (d, i) => colors(i))
        .style("stroke", "black")
        .style("stroke-width", 2)
        .attr("r", 0)
        .transition()
        .duration(150)
        .attr("r", 8)
        .transition()
        .duration(150)
        .attr("r", 6);
}

function draw_cluster_assignment(data) {
    svg.selectAll(".line")
        .data(data)
        .enter()
        .append("line")
        .attr("class", "line")
        .attr("x1", d => d.x)
        .attr("y1", d => d.y)
        .attr("x2", d => d.x)
        .attr("y2", d => d.y)
        .style("stroke", d => colors(d.cluster))
        .style("stroke-opacity", 0.4)
        .transition()
        .delay(d => Math.random() * 500)
        .duration(300)
        .attr("x2", d => centroids[d.cluster].x)
        .attr("y2", d => centroids[d.cluster].y)

    svg.selectAll(".data")
        .transition()
        .delay(d => Math.random() * 500)
        .duration(100)
        .style("fill", (d, i) => colors(d.cluster));
}

function draw_centroids_update(new_centroids) {
    svg.selectAll(".centroid")
       .data(new_centroids)
       .transition()
       .delay(750)
       .duration(500)
       .attr("cx", d => d.x)
       .attr("cy", d => d.y)

    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(750)
       .duration(500)
       .attr("x2", d => new_centroids[d.cluster].x)
       .attr("y2", d => new_centroids[d.cluster].y)
}

function remove_visual_annotations() {
    svg.selectAll(".line")
       .data(data)
       .transition()
       .delay(1500)
       .style("stroke-opacity", 0)
       .remove()

    svg.selectAll(".data")
       .data(data)
       .transition()
       .delay(1500)
       .style("fill", "black")

}

function select_centroids(data, k) {
    return d3.range(k).map(function() {return data[Math.floor(Math.random() * data.length)]});
}

function assign_cluster(data, centroids) {
    data.forEach(d => d.cluster = findClosestCentroid(d, centroids));
}

function compute_centroids(k) {
    centroids = d3.range(k).map(function(i) {
        const points_in_cluster = data.filter(d => d.cluster == i)
        return {
          x: d3.mean(points_in_cluster.map(d => d.x)),
          y: d3.mean(points_in_cluster.map(d => d.y))
        };
    });
    return centroids;
}

var colors = d3.scaleOrdinal().domain(d3.range(k)).range(d3.schemeCategory10);
let centroids = [];
draw_data(data, svg);

play_layer = svg.append("g"); 
play_layer.append("rect")
  .attr("width", "100%")
  .attr("height", "100%")
  .attr("fill", "white")
  .attr("fill-opacity", 0.5)
play_layer.append("path")
  .attr("d", d3.symbol().type(d3.symbolTriangle).size(4000))
  .attr("fill", "black")
  .attr("transform", function(d) { 
    return "rotate(-30) translate(50, 200)";
  });

function animate() {
    const delaunay = d3.Delaunay.from(centroids, d => d.x, d => d.y);
    const voronoi = delaunay.voronoi([0, 0, w, h]);
    let v = svg.selectAll(".voronoi")
        .data(centroids);
    v.enter()
        .append("path")
        .attr("class", "voronoi")
        .attr("d", (d, i) => voronoi.renderCell(i))
        .attr("fill-opacity", 0.25)
        .attr("fill", (d, i) => colors(i))
        .attr("stroke", "white")
        .attr("stroke-width", 0.5);
    v.exit().remove();
    v.transition()
        .duration(1000)
        .attr("d", (d, i) => voronoi.renderCell(i))

    assign_cluster(data, centroids);
    //draw_cluster_assignment(data);
    centroids = compute_centroids(k);
    draw_centroids_update(centroids);
    remove_visual_annotations();
    svg.transition().delay(2000).on("start", animate);
}

svg.on("click", function repeat(){
    play_layer.remove()
    centroids = select_centroids(data, k);
    draw_centroids(centroids, svg);
    animate()
});
</script>


<br>
Once the algorithm has converged it is possible to visualize the partitioning.
Do to so, we can generalize how we define the \\( k \\)
groups and include all possible elements rather
than only observed data points:
\\[ S_i = \\{x \in \mathbb{R}^d  ;\; ||x - c_i || \le || x - c_j ||, \; \forall 1 \le j \le k \\} \\]
These are called [Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram) and they look like this:
<figure>
  <img src="/assets/img/voronoi_k2.png" alt="Voronoi cells, k=2" style="max-width:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k3.png" alt="Voronoi cells, k=3" style="max-width:300px;display:inline-block;width:32%;">
  <img src="/assets/img/voronoi_k5.png" alt="Voronoi cells, k=5" style="max-width:300px;display:inline-block;width:32%;position: center;">
  <figcaption>Fig.2 - Voronoi cells with k=2 (left), k=3 (middle) and k=5 (right).</figcaption>
</figure>

## Python Implementation using Numpy

Although the algorithm could be implemented with a bunch of nested for-loops,
it can execute several orders of magnitude faster if we leverage matrix arithmetic.
Numpy {%cite harris2020array -f kmeans %} is an [open-source Python library](https://numpy.org/)
designed to ease the manipulation of vectors, matrix and arrays of any dimension.
The core operations are written in C, a low-level language, to achieve fast runtime.

You can use `conda` or `pip` package managers to install Numpy.
<br/> conda: `conda install numpy`
<br/> pip: `pip install numpy`

Let's start by importing Numpy and creating some synthetic data generated from 3 Gaussians.
```python
import numpy as np

blob1 = np.random.multivariate_normal(mean=[-3, 3], cov=[[3, 2], [2, 3]], size=100)
blob2 = np.random.multivariate_normal(mean=[5, 2], cov=[[2, 1], [1, 2]], size=100)
blob3 = np.random.multivariate_normal(mean=[0, -3], cov=[[2, 0], [0, 2]], size=100)
data = np.vstack([blob1, blob2, blob3])
```

The next step is to define a function for each step of the algorithm.
```python
def pick_centroids(data, k):
    indexes = np.random.choice(len(data), size=k, replace=False)
    centroids = data[indexes]
    return centroids

def assign_cluster(data, centroids):
    # Pairwise squared L2 distances. Shape [n, k]
    distances = ((data[:, np.newaxis] - centroids)**2).sum(axis=2)
    # find closest centroid index. Shape [n]
    clusters = np.argmin(distances, axis=1)
    return clusters

def update_centroids(data, clusters, k):
    # Mean positions of data within clusters
    centroids = [np.mean(data[clusters == i], axis=0) for i in range(k)]
    return np.array(centroids)
```

The final step is to glue everything together. For that, I use the following class:
```python
class KMeans:
    def __init__(self, k=3):
        self.k = k
        
    def fit(self, data, steps=20):
        self.centroids = pick_centroids(data, self.k)
        for step in range(steps):
            clusters = assign_cluster(data, self.centroids)
            self.centroids = update_centroids(data, clusters, self.k)
            
    def predict(self, data):
        return assign_cluster(data, self.centroids)
```

Finally, we can instantiate an object, train it and do the predictions.
```python
kmeans = KMeans(k=3)
kmeans.fit(data)
clusters = kmeans.predict(data)
```

If you have Matplotlib installed (`pip install matplotlib`) you can visualize the data and the result.

```python
import matplotlib.pyplot as plt
# Plot data points with cluster id as color
plt.scatter(data[:, 0], data[:, 1], c=clusters)
# Plot centroids
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c="red")
plt.show()
```
You should have something looking like this:

<figure>
  <img src="/assets/img/matplotlib.png" alt="Matplotlib display" style="max-width:350px;display:inline-block;width:90%;">
  <figcaption>Fig.3 - Matplotlib cluster visualization</figcaption>
</figure>

I recommend you to run the code step by step to build a better grasp of Numpy.
Read the [Numpy documentation about broadcasting operations](https://numpy.org/doc/stable/user/basics.broadcasting.html)
to understand how the pairwise distance is computed with a single line of code.

I suggest you do not use this code if it's not for educational purpose. Scikit-learn {%cite scikit-learn -f kmeans %}
is a very popular [open-source Python library](https://scikit-learn.org/stable/), build on top of Numpy,
which implements [the naive k-means algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=k%20means#sklearn.cluster.KMeans), its variants
and many more machine learning algorithms.


## Limitations and Variants

*K-means* strength is its conceptual simplicity. The logic but also the implementation
are intuitive and straightforwards. However, it is necessary to understand
that the algorithm was build around restritive hypothesis about the data
distribution. Some alternative algorithms overcome some of the limitations.

### Sensitivity to Initialization

The first drawback of k-means which can easily be observed after a few runs is the sensitivity
of the algorithm to the initial locations of the centroids.
**For each possible initilization corresponds a certain convergence speed and a final solution**.
Some solutions correspond to local minima and are far from optimal.

A naive way to address this sensitivity to initialization is to run the algorithm several
times and to keep the best model. Another solution to get around is to use expert knowledge
to hand-craft what could be a good initialization. A third solution is to **come up with
some heuristics to define a good initilization state. These are called seeding techniques**.

Many seeding heuristics have been developped overtime.
*K-means++* {% cite arthur2006k -f kmeans %} is one of the most popular one
and the default initiliazation method in Scikit-learn (and Matlab).
The motivation behind is that **spreading out the initial centroids
is beneficial**. Indeed, picking up far away centroids increase chances
that they belong in different clusters.

The *k-means++* seeding algorithm is as follow:
- Randomly select a first centroids among the data points.
- For all data points, compute the distance to the closest centroid \\( D(\textbf{x}) \\).
- Pick up another centroid from the set of data point, given the weighted
distribution proportional to the squared
distances: \\( p(\textbf{x}\_i)= \frac{ D(\textbf{x}_i)^2 }{ \sum\_{\textbf{x} \in X} D(\textbf{x})^2 } \\)
- Repeat the 2 previous steps \\( k-1 \\) times.

The benefits in terms of convergence speed and final error outweigth the computational overhead
for this initilization algorithm, see the original paper {% cite arthur2006k -f kmeans %} for details.
If you are looking for a thorough comparaison of seeding techniques, I recommend this recent review
{% cite franti2019much -f kmeans %}.
A good initialization of the centroids should be as close as possible to the
optimal solution. Thus finding a good initialization is not easier than solving the clustering problem itself.


### Computational Time Complexity

The assignment step time complexity is \\( O(nkd) \\)
where \\( n \\) is the number of samples, \\( k \\) is the number of clusters
and  \\( d \\) is the input space dimension. **This complexity is the consequence
of computing the pair-wise distance between all data points and all centroids**.
The update step has a time complexity of \\( O(nd) \\). The mean is computed along \\( d \\) dimensions for
\\( k \\) clusters, each containing an average of \\( n/k \\) data points.

The overall time complexity at each step of the Lloyd algorithm is therefore \\( O(nkd) \\).
If all these values increase by two-folds at once, then the algorithm will be eight times slower, not ideal.

There are some ways to mitigates this scaling problem. The first one is to **reduce the number
of dimensions by projection the data points into a smaller space** with
[principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)
or [independent component analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis).

A second approach to reducing the computational complexity is to use fewer
samples at each iteration. **Rather than working
with all data points, the algorithm only manipulates a subset
which randomly selected before each iteration.
This is called *mini-batch k-means***. Choosing the right
number of data point (batch size, noted \\( b \\)) could be a bit tricky.
We usually want the batch size to be far greater than the number of cluster
and much-lower than the number of samples (\\( k \ll b \ll n \\)).


### Sensitivity to Outliers

The centroids locations are defined as the mean position of the data points within each cluster.
However, **the mean is an estimator sensitive to outliers**. Outliers are data points which are 
significantly different from the other ones (out of distribution samples).
Fig.3 illustrates this issue where 2 data points out of distribution have a disproportionate impact on
the final result. **Not only some points are not assigned to the right cluster, but the decision
boundary is ultimately different**. 


<figure>
  <img src="/assets/img/outliers_off.png" alt="k-means nwithout outliers" style="max-width:300px;display:inline-block;width:48%;">
  <img src="/assets/img/outliers_on.png" alt="kmeans with outliers" style="max-width:300px;display:inline-block;width:48%;">
  <figcaption>Fig.4 - Without outliers (left) and with outliers (right)</figcaption>
</figure>

When the algorithm converges, it could be possible that one or more centroids deal with the outliers
and the other clusters model everything else. This would not be an efficient use of the clusters.
That being said, other centroids are no longer affected by outliers.

If possible, a solution could be to **preprocess the data by filtering out the outliers**.
A different remedy could be to **weight each data points**. During the update step,
weighted-mean is therefore used. *Weighted k-means* can reduce the impact of the outliers
and avoid the need for a hard decision in the preprocessing.
Finding a good weighting strategy is not easy and might take a few trials.

There are two more variants of *k-means* which are designed to reduce the influence of outliers.
For the first one, **the centroids are no longer defined as the mean locations but as the median position.
The algorithm is rightfully called *k-medians***. *K-means* minimize the within cluster-variance (L2-norm)
while *k-medians* minimize the absolute deviation (L1-norm). For k-medians the update rule becomes:

\\[ \forall 1\le i \le k, \,\, c^{t+1}_i = \text{median}( S^t_i ) \\]

The second adaptation enforces some restrictions to centroids. Rather than taking any possible locations, 
the **centroids are restricted to be data points. This algorithm is called *k-medoids***
(or PAM for Partitioning Around Medoids). A medoid is the most central representants in a cluster
that minimize the sum of distance to other data points. This distance metric makes *k-medoids*
more robust to outliers.
For *k-medoids*, the optimal solution is given by:

\\[ C_{best} = \underset{X}{\arg\min}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \lVert \textbf{x} - \textbf{c}_i \\rVert \\]

Finding the right medeoids requires a different algorithm (out of scope for
this post) and has a higher computational complexity of \\( O(k(n-k)^2) \\) at each iteration step.
See PAM, CLARA and CLARANS algorithms {% cite schubert2019faster -f kmeans %} if you are interested.

### Shape and Extend of Clusters

K-means, and all its variants, minimize distortion and optimize the clusters for compactness.
**The implicit assumption is that clusters are roughly spherical**
([isotropic](https://en.wikipedia.org/wiki/Isotropy), uniform in all directions).
Depending on the distribution of data points, some clusters might not be exactly spherical, but they
will **at least be convex partitions**. Convex means that for any two points in the same cluster,
the whole segment from one point to the other is fully contained inside the Voronoi cell.

Some good examples of failling cases are the concentric circles, the half-moons and the smiley
data distributions illustrated in the figure below. There is no way for *k-means* to isolate
the inner ring from the outer ring as the outer partition will not be convex.
Same problem on the 4 components of the smiley. For the two half-moons, the geometrical
properties of each cluster cannot be captured.

One partial solution is to overestimate the number of clusters and merge the partitions latter on.
However, that might not be easy for high-dimensional data and it will never really compensate if the data
distribution violate the initial assumption of spherical/convex clusters.

<figure>
  <img src="/assets/img/rings.png" alt="Rings dataset" style="max-width:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/halfmoons.png" alt="Half moons datasets" style="max-width:300px;display:inline-block;width:32.5%;">
  <img src="/assets/img/smiley.png" alt="Smiley datasets" style="max-width:300px;display:inline-block;width:32.5%;">
  <figcaption>Fig.4 - Partitions for non-convex and geometric clusters. Concentric&nbsp;rings&nbsp;(left, k=8), half&nbsp;moons&nbsp;(center,&nbsp;k=4) and smiley&nbsp;(right,&nbsp;k=12)&nbsp;datasets</figcaption>
</figure>

A further limitation is the relative size of clusters. By placing the decision boundaries halfway between 
centroids, **the underlying assumption is that cluster are evenly sized along that direction**.
The size is defined as the spatial extend (area, volume or [Lebesgue measure](https://en.wikipedia.org/wiki/Lebesgue_measure)) not the number of data points assigned inside the cluster.

Once again it's possible that some datasets with complex geometrical shapes
do not match this assumption. The perfect examples is the Mickey Mouse dataset.

<figure>
  <img src="/assets/img/mickey.png" alt="mickey dataset" style="max-width:300px;display:inline-block;width:49%;">
  <img src="/assets/img/tshape.png" alt="T shape datasets" style="max-width:300px;display:inline-block;width:49%;">
  <figcaption>Fig.4 - K-means partitions with anisotropic clusters and different&nbsp;c*variances.</figcaption>
</figure>

Finally, the *k-means* could fail if data arises from two blobs with similar size, but one generates far
fewer observations. This is somewhat related to the initilialization problem as selecting a centroid from
the underrepresented class is less likely. 

*K-means*, and its variances, only learns the central location of clusters, nothing more.
**It would be relevant to also learn the spatial extend of each cluster**.
**[Gaussian Mixture Model](https://en.wikipedia.org/wiki/Mixture_model)** (*GMM*)
was designed to capture the mean location and standard deviation of the data.

*GMM* computes the probability for all data points to belong to a cluster, for all clusters. 
This is called a soft-assignement in opposition to the hard-assignment to one and
only one cluster used by *k-means*. *GMM* is a powerful tool and there is a lot to say about it,
but that will be a story for another time.


### Optimal Number of Clusters

Last but not least, finding the optimal number of clusters could be quite tricky.
Priors or domain knowledge can be very handy for this task.
To this day, more than 30 methods has been published on this topic.
I will try to cover the most common methods.

#### The Elbow Method

First of all, we need to define a metric which tells how compact the clusters are.
A pertinent metric for this is the **with-in cluster sum squared error (SSE or WSS)**.
This is defined as the sum of squared distance between each point and its closest centroid.

\\[ SSE(k) = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 \\]

This function is strictly decreasing when applied on converged models. At first,
when we add clusters it gets easier to model the data. But after a certain point,
increasing the number of clusters brings little additional benefits
as it only split clusters into sub-groups or start modeling the noise.
**The *Elbow method* {% cite thorndike1953belongs -f kmeans %} is a heuristic to
find this point of diminishing return**.
It is done by plotting the evolution of the explained variance as the number 
of cluster grows. **The optimal number of cluster is located at the bend,
the elbow of the curve.**


<figure>
  <img src="/assets/img/elbow.png" alt="Explained variance" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.4 - Explained variance</figcaption>
</figure>

\\( SSE \\) is often applied once the model has converged.
However, it can also be used as stopping criteria for the algorithm.
If \\( SSE \\) is computed at the end of each iteration, a heuristic
could be stop the training when the \\( SSE \\) decreases less than a
certain threshold compared to the previous step.


#### Silhouette

TBC

#### Measures of Information

The *Elbow Method* is far from ideal. The elbow is not formally defined
and an visual inspection is necessary to localise it.
Consequently, the estimation of the optimal number of clusters can be subjective.
Not all SSE curves will have as sharp elbow like in the example above!

**A good estimator of the model performance should take into account the quality of the model
but should also penalise for the complexity of the model. Bayesian Information Criterion** (*BIC*)
{% cite schwarz1978estimating -f kmeans %} and **Akaike Information Criterion** (*AIC*)
{% cite akaike1974new -f kmeans %} measure the statistical quality of a model and respect this
parsimony principle (more parameters should not be added without necessity). For both
metrics, the lower the better.

For k-means, the simpliest expressions for \\( BIC \\) and \\( AIC \\) are defined as:
\begin{aligned}
 BIC &= \frac{SSE}{\hat\sigma^2} + kd\log(n) \\\\ 
 AIC &= \frac{SSE}{\hat\sigma^2} + 2kd 
\end{aligned}

\\( \hat\sigma^2 \\) is the average intra-cluster variance defined on a model
with a very high number of clusters {% cite friedman2001elements -f kmeans --label section --locator 7.7 %}:
\\( \hat\sigma^2 = \frac{SSE(k')}{k'},\; k'>k \\). It is worth computing \\( \hat\sigma^2 \\)
for several values of \\( k' \\). Small \\( k' \\) penalizes models with high number of clusters
more heavily

<figure>
  <img src="/assets/img/bic-aic.png" alt="kmeans BIC and AIC measure" style="max-width:680px;display:inline-block;width:103%;">
  <figcaption>Fig.4 - BIC and AIC</figcaption>
</figure>

From the above figure, we can see that curves for \\(k'=16\\) and \\(k'=20\\)
are overlapping, for both \\( BIC \\) and \\(AIC\\). This is due to the fact that
the average intra-cluster variances are nearly equal. In both case, and for both
metrics, the optimal number of cluster is 4.

The story and the conclusion is a bit different for \\( k'=12 \\).
The minimum for \\( BIC \\) is reached for \\( k = 2 \\) but the minimum 
for \\( AIC \\) is obtained with \\( k = 4 \\). This illustrates that small
\\(k'\\) leads to models with less clusters. It also shows that BIC
puts a higher penalty than \\(AIC\\).

<details>
  <summary style="background:#eee">This is not the whole story. Click to expand if you like equations!</summary>
  <div style="background:#eee;padding:5px">

<p>
\(BIC\) and \(AIC\) are originally defined as: 
<br/>
\( BIC = p\log(n) - 2 \log\left(L (M; X) \right) \) and \( AIC = 2p - 2 \log\left(L (M; X) \right) \)
<br/>where \( M \) is the model, \( p \) is the number of free-parameters of \( M \),
and \( L (M; X) \) is the model likelihood.
</p>

<p>
With k-means, we can heavily relies on the assumption that data arises
from identical isotropic blobs. Observed data points can be decomposed
as \( \textbf{x} = \mu + e, \; \forall \textbf{x} \in X \) where the \( \mu \)
is one of the centroids, \(\mu \in C \);
and the error follows a multivarite gaussians, \( e \sim \mathcal{N}(0,\,\sigma^{2}I_d)\).
</p>

<p>
Given a partition of the space, the probability to observe a data point \( \textbf{x} \) is therefore defined by:

\[ P(\textbf{x} \vert S) = \frac{n_i}{n} 
\frac{1}{\sqrt{2 \pi \sigma^2}^{d}} \exp \left( -\frac{\vert \vert \textbf{x} - \textbf{c}_i \vert \vert_2^2}{2\sigma^2} \right) \]
</p>


<p>
This can be injected into the log-likehood:

\[ \begin{aligned}
\log & L(S; X)  = \log \prod_{\textbf{x} \in X} P(\textbf{x} \vert S) = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \log P(\textbf{x} \vert S)
               \\ &= \sum_{i=1}^{k} \left[ n_i \log\left(\frac{n_i}{n}\right) -  \frac{\sum_{\textbf{x} \in S_i} \vert \vert \textbf{x} - \textbf{c}_i \vert \vert_2^2}{2\sigma^2}\right] - \frac{nd}{2}\log(2\pi\sigma^2)
\end{aligned} \]

If we consider that all <b>clusters are evenly balanced</b> and if we assume <b>\( \sigma^2 \)
to be fixed</b>, than \( BIC \) and \(AIC\) are equal to the previous equation
with an additive constant. Even if \( \sigma^2 \) is fixed, it's unknow
and need to be computed in a low-biais regime.
</p>

<p>
Now, <b>let's assume \( \sigma^2 \) is can be estimated from the data.</b>
The unbiaised estimator of the variance for each cluster gives:

\[ \forall 1\le i \le k, \; \hat\sigma_i^2 = \frac{1}{d(n_i - 1)} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 \]

Combined with the definition of SSE, we ends up with:

\[ SSE = \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 = \sum_{i=1}^k d(n_i - 1) \hat \sigma_i^2 \]
</p>

<p>
With the underlying assumption that all clusters have the same extend, we have \( \hat\sigma^2 = \hat \sigma_i^2 \). In conclusion,
the unbiaised estimation of the variance is: 

\[ \hat\sigma^2 = \frac{1}{d(n-k)}  \sum_{i=1}^{k} \sum_{\textbf{x} \in S_i} \vert\vert \textbf{x} - \textbf{c}_i \vert\vert^2 = \frac{SSE}{d(n-k)} \]
</p>

Going back to the log-likelihood formula, it can be simplified into:

\[ \begin{aligned}
\log &  L(S; X) =  \sum_{i=1}^{k} \left[ n_i \log\left(\frac{n_i}{n}\right) \right] - \frac{d(n-k)}{2} - \frac{nd}{2}\log(2\pi\sigma^2)
\end{aligned} \]

In the end, knowing that there are \( p=k\times d \) free parameters for k-means, the \( BIC \) and \( AIC \) expressions become:

\[ \begin{aligned}
 BIC & =(2n + dk)\log(n) + d(n-k) + nd\log(2\pi\sigma^2) - 2 \sum_{i=1}^{k} n_i \log\left(n_i\right)
\\ AIC & = 2n\log(n) + d(n+k) + nd\log(2\pi\sigma^2) - 2 \sum_{i=1}^{k} n_i \log\left(n_i\right)
\\ \text{with}&\; \sigma^2 = \frac{SSE}{d(n-k)}
\end{aligned} \]

There is still one limitation with these equations: they consider
the assignment as the true label for classification rather.
The uncertainty in the cluster assignment is taken into account.
I recommend this article {% cite hofmeyr2020degrees -f kmeans %} which
shows that the number of free parameters is underestimated.

</div></details>


#### Gap statistic?

{%cite tibshirani2001estimating -f kmeans %}

#### Dendrogram/dentrite?

{%cite calinski1974dendrite -f kmeans %}

#### Clustergram?

{%cite schonlau2002clustergram -f kmeans %}







